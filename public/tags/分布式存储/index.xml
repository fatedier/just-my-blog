<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fatedier blog </title>
    <link>http://blog.fatedier.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2016</rights>
    <updated>2015-11-24 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>codis 2.x版本环境搭建与测试</title>
          <link>http://blog.fatedier.com/2015/10/07/installation-and-testing-of-codis-version-two</link>
          <pubDate>Tue, 24 Nov 2015 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2015/10/07/installation-and-testing-of-codis-version-two</guid>
          <description>

&lt;p&gt;Codis 是一个分布式 Redis 解决方案, 对于上层的应用来说, 连接到 Codis Proxy 和连接原生的 Redis Server 没有明显的区别（有一些命令不支持），上层应用可以像使用单机的 Redis 一样，Codis 底层会处理请求的转发。Codis 支持不停机进行数据迁移, 对于前面的客户端来说是透明的, 可以简单的认为后面连接的是一个内存无限大的 Redis 服务。&lt;/p&gt;

&lt;h3 id=&#34;安装并启动-zookeeper:57de784f564f909e53fd43ba72fc074a&#34;&gt;安装并启动 zookeeper&lt;/h3&gt;

&lt;p&gt;codis 2.x 版本重度依赖于 zookeeper。&lt;/p&gt;

&lt;p&gt;从官网下载 &lt;a href=&#34;http://zookeeper.apache.org/releases.html&#34;&gt;zookeeper&lt;/a&gt;，解压安装。&lt;/p&gt;

&lt;p&gt;使用默认配置启动 zookeeper &lt;code&gt;sh ./bin/zkServer.sh start&lt;/code&gt;，监听地址为 &lt;code&gt;2181&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;下载安装-codis:57de784f564f909e53fd43ba72fc074a&#34;&gt;下载安装 codis&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;go get -d github.com/CodisLabs/codis&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;进入源码根目录 &lt;code&gt;cd $GOPATH/src/github.com/CodisLabs/codis&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;执行安装脚本 &lt;code&gt;./bootstrap.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注：这里第一步和第三步（会下载第三方库到本地）需要从 github copy 数据，鉴于网络环境的问题，时间会比较长。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;之后生成的可执行文件都在 &lt;code&gt;codis/bin&lt;/code&gt; 文件夹下。&lt;/p&gt;

&lt;h3 id=&#34;部署-codis-server:57de784f564f909e53fd43ba72fc074a&#34;&gt;部署 codis-server&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;codis-server&lt;/strong&gt; 基于 &lt;strong&gt;redis 2.8.21&lt;/strong&gt; 稍微进行了一些修改以支持原子性的迁移数据，具体用法和 redis 一致。&lt;/p&gt;

&lt;p&gt;将 &lt;code&gt;bin&lt;/code&gt; 文件夹下的 codis-server 拷贝到集群中每个节点，和 redis-server 的启动命令一样，指定配置文件，启动。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这里要注意 redis.conf 配置中需要设置 maxmemory，不然无法自动按照负载均衡的方式分配 slot（可以手动分配），推荐单台机器部署多个 redis 实例。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-server ./redis_conf/redis_6400.conf&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-dashboard:57de784f564f909e53fd43ba72fc074a&#34;&gt;启动 dashboard&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;dashboard&lt;/strong&gt; 既是 codis 集群的管理中心，又提供了一个人性化的 web 界面，方便查看统计信息以及对集群进行管理操作。&lt;/p&gt;

&lt;p&gt;启动 web 控制面板，注意这里要用到配置文件，不指定的话就是当前目录下的 config.ini，可以用 &lt;code&gt;-c&lt;/code&gt; 参数指定。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nohup ./bin/codis-config -c ./config.ini dashboard --addr=:18087 &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;初始化-slot:57de784f564f909e53fd43ba72fc074a&#34;&gt;初始化 slot&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot init&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;该命令会在 zookeeper 上创建 slot 相关信息。&lt;/p&gt;

&lt;h3 id=&#34;添加-group:57de784f564f909e53fd43ba72fc074a&#34;&gt;添加 group&lt;/h3&gt;

&lt;p&gt;每个 &lt;strong&gt;group&lt;/strong&gt; 只能有一个 &lt;strong&gt;master&lt;/strong&gt; 和多个 &lt;strong&gt;slave&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;命令格式： &lt;code&gt;codis-config -c ./config.ini server add &amp;lt;group_id&amp;gt; &amp;lt;redis_addr&amp;gt; &amp;lt;role&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;例如向 group 1 和 group 2 中各加入两个 codis-server 实例，一主一从。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 1 localhost:6379 master&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 1 localhost:6380 slave&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 2 localhost:6381 master&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 2 localhost:6382 slave&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 代表 group_id，必须为数字，且从 1 开始&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;分配-slot:57de784f564f909e53fd43ba72fc074a&#34;&gt;分配 slot&lt;/h3&gt;

&lt;h4 id=&#34;手动分配:57de784f564f909e53fd43ba72fc074a&#34;&gt;手动分配&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;codis-config -c ./config.ini slot range-set &amp;lt;slot_from&amp;gt; &amp;lt;slot_to&amp;gt; &amp;lt;group_id&amp;gt; &amp;lt;status&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;slot&lt;/strong&gt; 默认为 &lt;strong&gt;1024&lt;/strong&gt; 个，范围是 &lt;strong&gt;0 - 1023&lt;/strong&gt;，需要将这 1024 个 slot 分配到集群中不同的 group 中。&lt;/p&gt;

&lt;p&gt;例如将 1024 个 slot 平均分配到&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot range-set 0 511 1 online&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot range-set 512 1023 2 online&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;自动分配:57de784f564f909e53fd43ba72fc074a&#34;&gt;自动分配&lt;/h4&gt;

&lt;p&gt;在 dashboard 上可以自动分配 slot，会按照负载均衡的方式进行分配，不推荐使用，因为可能会造成大量数据的迁移。&lt;/p&gt;

&lt;p&gt;或者使用命令进行自动分配&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot rebalance&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-codis-proxy:57de784f564f909e53fd43ba72fc074a&#34;&gt;启动 codis-proxy&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-proxy -c ./config.ini -L ./log/proxy.log --cpu=8 --addr=10.10.100.1:19000 --http-addr=10.10.100.1:11000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：这里 &amp;ndash;addr 和 &amp;ndash;http-addr 不要填 0.0.0.0，要绑定一个具体的 ip，不然 zookeeper 中存的将是hostname，会导致 dashboard 无法连接。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;codis-proxy 是无状态的，可以部署多个，且用 go 编写，可以利用多核，建议 cpu 设置核心数的一半到2/3，19000 即为访问 redis 集群的端口，11000 为获取 proxy 相关状态的端口。&lt;/p&gt;

&lt;p&gt;之后使用 codis-config 将 codis-proxy 加入进来，也就是设置online（后来更新了一个版本，默认启动后即自动注册为online）&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bin/codis-config -c ./config.ini proxy online &amp;lt;proxy_name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;需要注意的是，启动 codis-proxy，会在 zookeeper 中注册一个 node，地址为 /zk/codis/db_test/fence，如果使用 kill -9 强行杀掉进程的话，这个会一直存在，需要手工删除。且 node 名称为 [hostname:port]，所以需要注意这个组合不能重复。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;主从切换:57de784f564f909e53fd43ba72fc074a&#34;&gt;主从切换&lt;/h3&gt;

&lt;p&gt;官方建议是手工操作，避免数据不一致的问题，但是没有自动容灾的话可用性太差。&lt;/p&gt;

&lt;p&gt;官方另外提供了一个工具，&lt;strong&gt;codis-ha&lt;/strong&gt;，这是一个通过 codis 开放的 api 实现自动切换主从的工具。该工具会在检测到 master 挂掉的时候将其下线并选择其中一个 slave 提升为 master 继续提供服务。&lt;/p&gt;

&lt;p&gt;这个工具不是很好用，如果 codis-ha 连接 dashboard 失败之后进程就会自动退出，需要手动重启或者使用 supervisor 拉起来。另外，当有机器被提升为 master 之后，其他 slave 的状态不会改变，还是从原 master 同步数据。原来的 master 重启之后处于 offline 状态，也需要手动加入 group 指定为 slave。也就是说有master 挂掉后，其余机器的状态需要手动修改。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-ha --codis-config=10.10.100.3:18087 --productName=common&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;10.10.100.14:18088&lt;/code&gt; 为 dashboard 所在机器的 ip 和端口。&lt;/p&gt;

&lt;h3 id=&#34;旧数据的迁移:57de784f564f909e53fd43ba72fc074a&#34;&gt;旧数据的迁移&lt;/h3&gt;

&lt;p&gt;官方提供了一个 &lt;strong&gt;redis-port&lt;/strong&gt; 工具可以将旧 redis 中的数据实时迁移到 codis 集群中，之后需要修改各服务配置文件，重启服务，指向 codis 集群即可。&lt;/p&gt;

&lt;h3 id=&#34;性能测试:57de784f564f909e53fd43ba72fc074a&#34;&gt;性能测试&lt;/h3&gt;

&lt;p&gt;测试环境： 24核 2.1GHz，4个redis实例&lt;/p&gt;

&lt;h4 id=&#34;不启用-pipeline:57de784f564f909e53fd43ba72fc074a&#34;&gt;不启用 pipeline&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;GET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MSET&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;redis单机&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58997.05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58651.02&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33557.05&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis1核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42973.79&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33003.30&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12295.58&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis4核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44208.66&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39936.10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21743.86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39478.88&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;23052.10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24679.17&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis12核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28943.56&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24224.81&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21376.66&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核2proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62085.65&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;68964.40&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48298.74&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;pipeline-100:57de784f564f909e53fd43ba72fc074a&#34;&gt;pipeline = 100&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;GET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MSET&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;redis单机&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;259067.36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;340136.06&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40387.72&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis1核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;158982.52&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;166112.95&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15199.88&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis4核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;491159.12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;403551.25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40157.42&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;518134.72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;537634.38&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58156.44&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis12核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;520833.34&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500000.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53418.80&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核2proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;529812.81&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;607041.47&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62872.28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过测试可以看出，使用 codis 会在性能上比原来直接使用 redis 会有所下降，但是优势就在于可以通过横向扩展（加机器）的方式去提高 redis 的存储容量以及并发量。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Redis集群调研</title>
          <link>http://blog.fatedier.com/2015/09/15/redis-cluster-survey</link>
          <pubDate>Tue, 15 Sep 2015 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2015/09/15/redis-cluster-survey</guid>
          <description>

&lt;p&gt;Redis作为一个使用场景很高的NoSQL数据库，支持了较为丰富的数据类型，相比于其他关系型数据库在性能方面优势明显。互联网公司通常更加倾向于将一些热点数据放入Redis中来承载高吞吐量的访问。&lt;/p&gt;

&lt;p&gt;单机Redis在普通的服务器上通常ops上限在5w左右，开启pipeline的情况下在20-30w左右。对于大多数中小公司来说，通常单机的Redis已经足够，最多根据不同业务分散到多台Redis。&lt;/p&gt;

&lt;h3 id=&#34;为什么需要集群:51fa3b005f880190963d473247382d62&#34;&gt;为什么需要集群&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Redis单线程特性，多请求顺序执行，单个耗时的操作会阻塞后续的操作&lt;/li&gt;
&lt;li&gt;单机内存有限&lt;/li&gt;
&lt;li&gt;某些特殊业务，带宽压力较大&lt;/li&gt;
&lt;li&gt;单点问题，缺乏高可用性&lt;/li&gt;
&lt;li&gt;不能动态扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Redis集群的目标就是为了实现高可用性，避免性能瓶颈，可动态扩容，易于做监控告警。&lt;/p&gt;

&lt;h3 id=&#34;三种主流的集群解决方案:51fa3b005f880190963d473247382d62&#34;&gt;三种主流的集群解决方案&lt;/h3&gt;

&lt;h4 id=&#34;客户端静态分片:51fa3b005f880190963d473247382d62&#34;&gt;客户端静态分片&lt;/h4&gt;

&lt;p&gt;通常需要 smart-client 支持，在业务程序端根据预先设置的路由规则进行分片，从而实现对多个redis实例的分布式访问。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-jedis.png&#34; alt=&#34;jedis&#34; /&gt;&lt;/p&gt;

&lt;p&gt;鉴于redis本身的高性能，并且有一些设计良好的第三方库，例如java开发者可以使用jedis，所以很多小公司使用此方案。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt; 相比于使用代理，减少了一层网络传输的消耗，效率较高。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt; 当redis实例需要扩容或切换的情况下，需要修改业务端的程序，较为麻烦。并且需  要维护各个语言的客户端版本，如果要升级客户端成本也会比较高。出现故障时难以及时定位问题。（有些smart-client借助于zookeeper维护客户端访问redis实例的一致性）&lt;/p&gt;

&lt;h4 id=&#34;proxy分片:51fa3b005f880190963d473247382d62&#34;&gt;Proxy分片&lt;/h4&gt;

&lt;p&gt;通过统一的代理程序访问多个redis实例，比如业内曾广泛使用的 twemproxy 以及 豌豆荚开源的 codis。（twemproxy是twitter开源的一个redis和memcache代理服务器，只用于作为简单的代理中间件，目前twitter内部已经不再使用）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt; 业务程序端只需要使用普通的api去访问代理程序即可。各种组件分离，以后升级较为容易。也避免了客户端需要维持和每个redis实例的长连接导致连接数过多。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt; 增加了一层中间件，增加了网络和数据处理的消耗，性能下降。&lt;/p&gt;

&lt;h4 id=&#34;official-redis-cluster:51fa3b005f880190963d473247382d62&#34;&gt;Official Redis Cluster&lt;/h4&gt;

&lt;p&gt;Redis3.0之后的版本开始正式支持 redis cluster，核心目标是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;性能：&lt;/strong&gt;Redis作者比较看重性能，增加集群不能对性能有较大影响，所以Redis采用了P2P而非Proxy方式、异步复制、客户端重定向等设计，而牺牲了部分的一致性、使用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;水平扩展：&lt;/strong&gt;官方文档中称目标是能线性扩展到1000结点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可用性：&lt;/strong&gt;集群具有了以前Sentinel的监控和自动Failover能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;基于twemproxy的redis集群环境:51fa3b005f880190963d473247382d62&#34;&gt;基于twemproxy的redis集群环境&lt;/h3&gt;

&lt;h4 id=&#34;整体架构图:51fa3b005f880190963d473247382d62&#34;&gt;整体架构图&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-twemproxy-architecture.png&#34; alt=&#34;twemproxy_architecture&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;twemproxy的特点:51fa3b005f880190963d473247382d62&#34;&gt;twemproxy的特点&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;支持失败的节点自动摘除（仅作为缓存时）&lt;/li&gt;
&lt;li&gt;所有的key通过一致性哈希算法分布到集群中所有的redis实例中&lt;/li&gt;
&lt;li&gt;代理与每个redis实例维持长连接，减少客户端和redis实例的连接数&lt;/li&gt;
&lt;li&gt;代理是无状态的，可以任意部署多套，避免单点问题&lt;/li&gt;
&lt;li&gt;默认启用pipeline，连接复用，提高效率，性能损失在 10% - 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;集群组件:51fa3b005f880190963d473247382d62&#34;&gt;集群组件&lt;/h4&gt;

&lt;p&gt;由于twemproxy本身只是简单的代理，所以需要依赖于一些其他的程序组件。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Redis Sentinel：&lt;/strong&gt; 管理主从备份，用于主从切换，当主服务器挂掉后，自动将从服务器提升为主服务器&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Redis-Twemproxy Agent：&lt;/strong&gt; nodejs写的一个监控程序，用于监听 redis-sentinel 的 master 切换事件，并且及时更新twemproxy的配置文件后将其重新启动&lt;/p&gt;

&lt;h4 id=&#34;why-not-twemproxy:51fa3b005f880190963d473247382d62&#34;&gt;Why not Twemproxy&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;虽然使用 c 开发，性能损失较小，但同样是单线程。所以基本上twemproxy的数量需要和后端redis实例一样甚至更多才能充分发挥多实例的并发能力，造成运维困难。&lt;/li&gt;
&lt;li&gt;twemproxy更改配置文件需要重新启动，比较坑，需要修改代码使其支持动态加载。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无法动态扩容&lt;/strong&gt;，如果需要实现这个功能，要么自己写迁移脚本，手动迁移，比较繁琐，还会影响到当前服务的正常运行。或者二次开发，增加对zookeeper的依赖，将redis节点信息以及hash域相关的数据存储在zookeeper上，然后增加动态迁移数据的模块，可以在不影响现有服务运行的情况下完成增删实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;redis-cluster:51fa3b005f880190963d473247382d62&#34;&gt;Redis Cluster&lt;/h3&gt;

&lt;h4 id=&#34;数据分布-预分片:51fa3b005f880190963d473247382d62&#34;&gt;数据分布：预分片&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-redis-cluster-pre-sharding.png&#34; alt=&#34;redis-cluster-pre-sharding&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;预先分配好 16384 个slot&lt;/li&gt;
&lt;li&gt;slot 和 server 的映射关系存储每一个 server 的路由表中&lt;/li&gt;
&lt;li&gt;根据 CRC16(key) mod 16384 的值，决定将一个key放到哪一个slot中&lt;/li&gt;
&lt;li&gt;数据迁移时就是调整 slot 的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;架构-去中心化:51fa3b005f880190963d473247382d62&#34;&gt;架构：去中心化&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-redis-cluster-architecture.png&#34; alt=&#34;redis-cluster-architecture&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无中心结构，每个节点都保存数据和整个集群的状态。&lt;/li&gt;
&lt;li&gt;采用 gossip 协议传播信息以及发现新节点（最终一致性）。

&lt;ul&gt;
&lt;li&gt;每个节点都和其他所有节点连接，并保持活跃。&lt;/li&gt;
&lt;li&gt;PING/PONG：心跳，附加上自己以及一些其他节点数据，每个节点每秒随机PING几个节点。会选择那些超过cluster-node-timeout一半的时间还未PING过或未收到PONG的节点。&lt;/li&gt;
&lt;li&gt;UPDATE消息：计数戳，如果收到server的计数为3，自己的为4，则发UPDATE更新对方路由表，反之更新自己的路由表，最终集群路由状态会和计数戳最大的实例一样。&lt;/li&gt;
&lt;li&gt;如果 cluster-node-timeout 设置较小，或者节点较多，数据传输量将比较可观。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Broadcast：有状态变动时先broadcast，后PING； 发布/订阅。&lt;/li&gt;
&lt;li&gt;Redis node 不作为client请求的代理（不转发请求），client根据node返回的错误信息重定向请求?（需要 smart-client 支持），所以client连接集群中任意一个节点都可以。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;可用性-master-slave:51fa3b005f880190963d473247382d62&#34;&gt;可用性：Master-Slave&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;每个Redis Node可以有一个或者多个Slave，当Master挂掉时，选举一个Slave形成新的Master。&lt;/li&gt;
&lt;li&gt;Master Slave 之间异步复制（可能会丢数据）。&lt;/li&gt;
&lt;li&gt;采用 gossip 协议探测其他节点存活状态，超过 cluster-node-timeout，标记为 PFAIL，PING中附加此数据。当 Node A发现半数以上master将失效节点标记为PFAIL，将其标记为FAIL，broadcast FAIL。&lt;/li&gt;
&lt;li&gt;各 slave 等待一个随机时间后 发起选举，向其他 master broadcast，半数以上同意则赢得选举否则发起下一次选举&lt;/li&gt;
&lt;li&gt;当 slave 成为 master，先broadcast，后持续PING，最终集群实例都获知此消息&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;存在的问题:51fa3b005f880190963d473247382d62&#34;&gt;存在的问题&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Gossip协议通信开销&lt;/li&gt;
&lt;li&gt;严重依赖于smart-client的成熟度

&lt;ul&gt;
&lt;li&gt;如果smart-client支持缓存slot路由，需要额外占用内存空间，为了效率需要建立和所有 redis server 的长连接（每一个使用该库的程序都需要建立这么多连接）。&lt;/li&gt;
&lt;li&gt;如果不支持缓存路由信息，会先访问任意一台 redis server，之后重定向到新的节点。&lt;/li&gt;
&lt;li&gt;需要更新当前所有的client。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;官方只提供了一个ruby程序 redis-trib 完成集群的所有操作，缺乏监控管理工具，很难清楚目前集群的状态&lt;/li&gt;
&lt;li&gt;数据迁移以Key为单位，速度较慢&lt;/li&gt;
&lt;li&gt;某些操作不支持，MultiOp和Pipeline都被限定在命令中的所有Key必须都在同一Slot内&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;codis:51fa3b005f880190963d473247382d62&#34;&gt;Codis&lt;/h3&gt;

&lt;h4 id=&#34;what-is-codis:51fa3b005f880190963d473247382d62&#34;&gt;What is Codis ？&lt;/h4&gt;

&lt;p&gt;Go语言开发的分布式 Redis 解决方案，对于上层的应用来说，访问 codis 和原生的 redis server 没有明显区别（不支持发布订阅等某些命令，支持 pipeline）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-codis-architecture.png&#34; alt=&#34;codis-architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Codis由四部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Codis Proxy (codis-proxy)&lt;/li&gt;
&lt;li&gt;Codis Dashboard (codis-config)&lt;/li&gt;
&lt;li&gt;Codis Redis (codis-server)&lt;/li&gt;
&lt;li&gt;ZooKeeper/Etcd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;codis-proxy 是客户端连接的 Redis 代理服务, codis-proxy 本身实现了 Redis 协议, 表现得和一个原生的 Redis 没什么区别 (就像 Twemproxy), 对于一个业务来说, 可以部署多个 codis-proxy, codis-proxy 本身是无状态的。&lt;/p&gt;

&lt;p&gt;codis-config 是 Codis 的管理工具, 支持包括, 添加/删除 Redis 节点, 添加/删除 Proxy 节点, 发起数据迁移等操作. codis-config 本身还自带了一个 http server, 会启动一个 dashboard, 用户可以直接在浏览器上观察 Codis 集群的运行状态。&lt;/p&gt;

&lt;p&gt;codis-server 是 Codis 项目维护的一个 Redis 分支, 基于 2.8.21 开发, 加入了 slot 的支持和原子的数据迁移指令. Codis 上层的 codis-proxy 和 codis-config 只能和这个版本的 Redis 交互才能正常运行。&lt;/p&gt;

&lt;p&gt;Codis 依赖 ZooKeeper 来存放数据路由表和 codis-proxy 节点的元信息, codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 codis-proxy。&lt;/p&gt;

&lt;p&gt;Codis 支持按照 Namespace 区分不同的产品, 拥有不同的 product name 的产品, 各项配置都不会冲突。&lt;/p&gt;

&lt;h4 id=&#34;整体设计:51fa3b005f880190963d473247382d62&#34;&gt;整体设计&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;预分片，1024 slot， key =&amp;gt; crc32(key)%1024&lt;/li&gt;
&lt;li&gt;proxy无状态，便于负载均衡，启动时在 Zookeeper 上注册一个临时节点，方便做 HA&lt;/li&gt;
&lt;li&gt;Redis 只作为存储引擎&lt;/li&gt;
&lt;li&gt;Go语言开发，可以充分利用多核，不必像 twemproxy 一样部署很多套&lt;/li&gt;
&lt;li&gt;性能损失，在不开启pipeline的情况下会损失大概40%，通过加实例线性扩展&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>
