<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fatedier blog </title>
    <link>http://blog.fatedier.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2016</rights>
    <updated>2016-07-06 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>InfluxDB 与 OpenTSDB 对比测试</title>
          <link>http://blog.fatedier.com/2016/07/06/test-influxdb-and-opentsdb</link>
          <pubDate>Wed, 06 Jul 2016 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2016/07/06/test-influxdb-and-opentsdb</guid>
          <description>

&lt;p&gt;通过调研，在时间序列数据库的选择上，从社区活跃度，易用程度，综合性能上来看比较合适的就是 OpenTSDB 和 InfluxDB，所以对这两个数据库进行了一个简单测试。&lt;/p&gt;

&lt;h3 id=&#34;时间序列数据库热度排名:4dfacd6c1d774692305fdb3325c74808&#34;&gt;时间序列数据库热度排名&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2016/2016-07-06-test-influxdb-and-opentsdb-db-rank.png&#34; alt=&#34;db-rank&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;测试环境:4dfacd6c1d774692305fdb3325c74808&#34;&gt;测试环境&lt;/h3&gt;

&lt;p&gt;青云 centos7&lt;/p&gt;

&lt;p&gt;4 cpu，8GB RAM&lt;/p&gt;

&lt;h3 id=&#34;测试样本:4dfacd6c1d774692305fdb3325c74808&#34;&gt;测试样本&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;metric&lt;/strong&gt; 从 cpu_load1 - cpu_load10 共10个。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tags&lt;/strong&gt; 只有一个，host_id 为 1-100。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;time&lt;/strong&gt; 为 1467000000 - 1467010000，各维度每秒生成1条数据。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;value&lt;/strong&gt; 取值为 0.1-0.9。&lt;/p&gt;

&lt;p&gt;合计数据量为 10 * 100 * 10000 = 1000w。&lt;/p&gt;

&lt;h3 id=&#34;查询测试用例:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询测试用例&lt;/h3&gt;

&lt;h4 id=&#34;查询1-单条查询:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询1：单条查询&lt;/h4&gt;

&lt;p&gt;指定 metricName 以及 host_id，对 time 在 1467000000 和 1467005000 内的数据按照每3分钟的粒度进行聚合计算平均值。&lt;/p&gt;

&lt;p&gt;例如 InfluxDB 中的查询语句&lt;/p&gt;

&lt;p&gt;&lt;code&gt;select mean(value) from cpu_load1 where host_id = &#39;1&#39; and time &amp;gt; 1467000000000000000 and time &amp;lt; 1467005000000000000 group by time(3m)&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;查询2-批量10条不同-metricname-查询:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询2：批量10条不同 metricName 查询&lt;/h4&gt;

&lt;p&gt;单条查询的基础上修改成不同的 metricName&lt;/p&gt;

&lt;h3 id=&#34;influxdb:4dfacd6c1d774692305fdb3325c74808&#34;&gt;InfluxDB&lt;/h3&gt;

&lt;p&gt;并发数 50 通过 http 写入，每次 100 条数据&lt;/p&gt;

&lt;h4 id=&#34;资源占用:4dfacd6c1d774692305fdb3325c74808&#34;&gt;资源占用&lt;/h4&gt;

&lt;p&gt;cpu 使用率维持在 100% 左右，耗时 1m58s，约 84746/s&lt;/p&gt;

&lt;p&gt;磁盘占用 70MB&lt;/p&gt;

&lt;h4 id=&#34;查询1:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询1&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Num1: 0.010s&lt;/li&gt;
&lt;li&gt;Num2: 0.010s&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;查询2:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询2&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Num 1: 0.029s&lt;/li&gt;
&lt;li&gt;Num 2: 0.021s&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;opentsdb:4dfacd6c1d774692305fdb3325c74808&#34;&gt;OpenTSDB&lt;/h3&gt;

&lt;p&gt;并发数 50 通过http写入，每次 100 条数据&lt;/p&gt;

&lt;h4 id=&#34;资源占用-1:4dfacd6c1d774692305fdb3325c74808&#34;&gt;资源占用&lt;/h4&gt;

&lt;p&gt;cpu 开始时跑满，之后在250%左右 耗时 2m16s，约 73529/s&lt;/p&gt;

&lt;p&gt;磁盘占用 1.6GB，由于是简单部署，这里 HBase 没有启用 lzo 压缩，据说压缩之后只需要占用原来 &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; 的空间，也就是 320MB。&lt;/p&gt;

&lt;h4 id=&#34;查询1-1:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询1&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Num 1: 0.285s&lt;/li&gt;
&lt;li&gt;Num 2: 0.039s&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;查询2-1:4dfacd6c1d774692305fdb3325c74808&#34;&gt;查询2&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Num 1: 0.111s&lt;/li&gt;
&lt;li&gt;Num 2: 0.040s&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;总结:4dfacd6c1d774692305fdb3325c74808&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;一开始是准备在本地的一个2核2GB的虚拟机里进行测试，InfluxDB 虽然比较慢，但是测试完成，而 OpenTSDB 测试过程中，要么 zookeeper 出现故障，要么 Hbase 异常退出，要么无法正常写入数据，始终无法完成测试。更换成配置更高的青云服务器后，两者都能正常完成测试。&lt;/p&gt;

&lt;p&gt;在单机部署上，InfluxDB 非常简单，一两分钟就可以成功运行，而 OpenTSDB 需要搭建 Hbase，创建 TSD 用到的数据表，配置 JAVA 环境等，相对来说更加复杂。&lt;/p&gt;

&lt;p&gt;资源占用方面，InfluxDB 都要占据优势，cpu 消耗更小，磁盘占用更是小的惊人。&lt;/p&gt;

&lt;p&gt;查询速度，由于测试样本数据量还不够大，速度都非常快，可以看到 InfluxDB 的查询在 10ms 这个数量级，而 OpenTSDB 则慢了接近 10 倍，第二次查询时，由于缓存的原因，OpenTSDB 的查询速度也相当快。&lt;/p&gt;

&lt;p&gt;集群方面，目前 InfluxDB 还没有比较好的解决方案，而 OpenTSDB 基于 HBase，这一套集群方案已经被很多大公司采用，稳定运行。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>时间序列数据库调研之InfluxDB</title>
          <link>http://blog.fatedier.com/2016/07/05/research-of-time-series-database-influxdb</link>
          <pubDate>Tue, 05 Jul 2016 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2016/07/05/research-of-time-series-database-influxdb</guid>
          <description>

&lt;p&gt;基于 Go 语言开发，社区非常活跃，项目更新速度很快，日新月异，关注度高。&lt;/p&gt;

&lt;h3 id=&#34;测试版本:441ef2dfb3e97650c90da967029aaee7&#34;&gt;测试版本&lt;/h3&gt;

&lt;p&gt;1.0.0_beta2-1&lt;/p&gt;

&lt;h3 id=&#34;安装部署:441ef2dfb3e97650c90da967029aaee7&#34;&gt;安装部署&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;wget https://dl.influxdata.com/influxdb/releases/influxdb-1.0.0_beta2.x86_64.rpm&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo yum localinstall influxdb-1.0.0_beta2.x86_64.rpm&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;配置文件路径为 &lt;code&gt;/etc/influxdb/influxdb.conf&lt;/code&gt;，修改后启动服务&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo service influxdb start&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;特点:441ef2dfb3e97650c90da967029aaee7&#34;&gt;特点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;可以设置metric的保存时间。&lt;/li&gt;
&lt;li&gt;支持通过条件过滤以及正则表达式删除数据。&lt;/li&gt;
&lt;li&gt;支持类似 sql 的语法。&lt;/li&gt;
&lt;li&gt;可以设置数据在集群中的副本数。&lt;/li&gt;
&lt;li&gt;支持定期采样数据，写入另外的measurement，方便分粒度存储数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;概念:441ef2dfb3e97650c90da967029aaee7&#34;&gt;概念&lt;/h3&gt;

&lt;h4 id=&#34;数据格式-line-protocol:441ef2dfb3e97650c90da967029aaee7&#34;&gt;数据格式 Line Protocol&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;measurement[,tag_key1=tag_value1...] field_key=field_value[,field_key2=field_value2] [timestamp]

cpu_load,host_id=1 value=0.1 1434055562000000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相比于 JSON 格式，无需序列化，更加高效。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;measurement: metric name，例如 cpu_load。&lt;/li&gt;
&lt;li&gt;field-key, field-value: 通常用来存储数据，类似 opentsdb 中的 value=0.6，但是支持各种类型，数据存储时不会进行索引，每条数据必须拥有一个 field-key，如果使用 field-key 进行过滤，需要遍历一遍所有数据。&lt;/li&gt;
&lt;li&gt;tags-key, tags-value: 和 field-key 类似，但是会进行索引，方便查询时用于过滤条件。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;series:441ef2dfb3e97650c90da967029aaee7&#34;&gt;Series&lt;/h4&gt;

&lt;p&gt;measurement, tag set, retention policy 相同的数据集合算做一个 series。&lt;/p&gt;

&lt;p&gt;假设 cpu_load 有两个 tags，host_id 和 name，host_id 的数量为 100，name 的数量为 200，则 series 基数为 100 * 200 = 20000。&lt;/p&gt;

&lt;h4 id=&#34;数据存储:441ef2dfb3e97650c90da967029aaee7&#34;&gt;数据存储&lt;/h4&gt;

&lt;p&gt;measurements, tag keys, field keys，tag values 全局存一份。&lt;/p&gt;

&lt;p&gt;field values 和 timestamps 每条数据存一份。&lt;/p&gt;

&lt;h4 id=&#34;retention-policy:441ef2dfb3e97650c90da967029aaee7&#34;&gt;Retention Policy&lt;/h4&gt;

&lt;p&gt;保留策略包括设置数据保存的时间以及在集群中的副本个数。&lt;/p&gt;

&lt;p&gt;默认的 RP 为 &lt;strong&gt;default&lt;/strong&gt;，保存时间不限制，副本个数为 1，默认 RP 是可以修改的，并且我们可以创建新的 RP。&lt;/p&gt;

&lt;h4 id=&#34;continuous-query:441ef2dfb3e97650c90da967029aaee7&#34;&gt;Continuous Query&lt;/h4&gt;

&lt;p&gt;CQ 是预先配置好的一些查询命令，&lt;strong&gt;SELECT&lt;/strong&gt; 语句必须包含 &lt;strong&gt;GROUP BY time()&lt;/strong&gt;，influxdb 会定期自动执行这些命令并将查询结果写入指定的另外的 measurement 中。&lt;/p&gt;

&lt;p&gt;利用这个特性并结合 RP 我们可以方便地保存不同粒度的数据，根据数据粒度的不同设置不同的保存时间，这样不仅节约了存储空间，而且加速了时间间隔较长的数据查询效率，避免查询时再进行聚合计算。&lt;/p&gt;

&lt;h4 id=&#34;shard:441ef2dfb3e97650c90da967029aaee7&#34;&gt;Shard&lt;/h4&gt;

&lt;p&gt;Shard 这个概念并不对普通用户开放，实际上是 InfluxDB 将连续一段时间内的数据作为一个 shard 存储，根据数据保存策略来决定，通常是保存1天或者7天的数据。例如如果保存策略 RP 是无限制的话，shard 将会保存7天的数据。这样方便之后的删除操作，直接关闭下层对应的一个数据库即可。&lt;/p&gt;

&lt;h3 id=&#34;存储引擎:441ef2dfb3e97650c90da967029aaee7&#34;&gt;存储引擎&lt;/h3&gt;

&lt;p&gt;从 LevelDB（LSM Tree），到 BoltDB（mmap B+树），现在是自己实现的 TSM Tree 的算法，类似 LSM Tree，针对 InfluxDB 的使用做了特殊优化。&lt;/p&gt;

&lt;h4 id=&#34;leveldb:441ef2dfb3e97650c90da967029aaee7&#34;&gt;LevelDB&lt;/h4&gt;

&lt;p&gt;LevelDB 底层使用了 LSM Tree 作为数据结构，用于存储大量的 key 值有序的 K-V 数据，鉴于时序数据的特点，只要将时间戳放入 key 中，就可以非常快速的遍历指定时间范围内的数据。LSM Tree 将大量随机写变成顺序写，所以拥有很高的写吞吐量，并且 LevelDB 内置了压缩功能。&lt;/p&gt;

&lt;p&gt;数据操作被先顺序写入 WAL 日志中，成功之后写入内存中的 MemTable，当 MemTable 中的数据量达到一定阀值后，会转换为 Immutable MemTable，只读，之后写入 SSTable。SSTable 是磁盘上只读的用于存储有序键值对的文件，并且会持续进行合并，生成新的 SSTable。在 LevelDB 中是分了不同层级的 SSTable 用于存储数据。&lt;/p&gt;

&lt;p&gt;LevelDB 不支持热备份，它的变种 RocksDB 和 HyperLevelDB 实现了这个功能。&lt;/p&gt;

&lt;p&gt;最严重的问题是由于 InfluxDB 通过 shard 来组织数据，每一个 shard 对应的就是一个 LevelDB 数据库，而由于 LevelDB 的底层存储是大量 SSTable 文件，所以当用户需要存储长时间的数据，例如几个月或者一年的时候，会产生大量的 shard，从而消耗大量文件描述符，将系统资源耗尽。&lt;/p&gt;

&lt;h4 id=&#34;boltdb:441ef2dfb3e97650c90da967029aaee7&#34;&gt;BoltDB&lt;/h4&gt;

&lt;p&gt;之后 InfluxDB 采用了 BoltDB 作为数据存储引擎。BoltDB 是基于 LMDB 使用 Go 语言开发的数据库。同 LevelDB 类似的是，都可以用于存储 key 有序的 K-V 数据。&lt;/p&gt;

&lt;p&gt;虽然采用 BoltDB 的写效率有所下降，但是考虑到用于生产环境需要更高的稳定性，BoltDB 是一个合适的选择，而且 BoltDB 使用纯 Go 编写，更易于跨平台编译部署。&lt;/p&gt;

&lt;p&gt;最重要的是 BoltDB 的一个数据库存储只使用一个单独的文件。Bolt 还解决了热备的问题，很容易将一个 shard 从一台机器转移到另外一台。&lt;/p&gt;

&lt;p&gt;但是当数据库容量达到数GB级别时，同时往大量 series 中写入数据，相当于是大量随机写，会造成 IOPS 上升。&lt;/p&gt;

&lt;h4 id=&#34;tsm-tree:441ef2dfb3e97650c90da967029aaee7&#34;&gt;TSM Tree&lt;/h4&gt;

&lt;p&gt;TSM Tree 是 InfluxDB 根据实际需求在 LSM Tree 的基础上稍作修改优化而来。&lt;/p&gt;

&lt;h5 id=&#34;wal:441ef2dfb3e97650c90da967029aaee7&#34;&gt;WAL&lt;/h5&gt;

&lt;p&gt;每一个 shard 对应底层的一个数据库。每一个数据库有自己的 WAL 文件，压缩后的元数据文件，索引文件。&lt;/p&gt;

&lt;p&gt;WAL 文件名类似 &lt;code&gt;_000001.wal&lt;/code&gt;，数字递增，每达到 2MB 时，会关闭此文件并创建新的文件，有一个写锁用于处理多协程并发写入的问题。&lt;/p&gt;

&lt;p&gt;可以指定将 WAL 从内存刷新到磁盘上的时间，例如30s，这样会提高写入性能，同时有可能会丢失这30s内的数据。&lt;/p&gt;

&lt;p&gt;每一个 WAL 中的条目遵循 TLV 的格式，1字节用于表示类型（points，new fields，new series，delete），4字节表示 block 的长度，后面则是具体压缩后的 block 内容。WAL 文件中得内容在内存中会进行缓存，并且不会压缩，每一个 point 的 key 为 measurement, tagset 以及 unique field，每一个 field 按照自己的时间顺序排列。&lt;/p&gt;

&lt;p&gt;查询操作将会去 WAL 以及索引中查询，WAL 在内存中缓存有一个读写锁进行控制。删除操作会将缓存中的key删除，同时在 WAL 文件中进行记录并且在内存的索引中进行删除标记。&lt;/p&gt;

&lt;h5 id=&#34;data-files-sstables:441ef2dfb3e97650c90da967029aaee7&#34;&gt;Data Files(SSTables)&lt;/h5&gt;

&lt;p&gt;这部分 InfluxDB 自己定义了特定的数据结构，将时间戳编码到了 DataFiles 中，进行了相对于时间序列数据的优化。&lt;/p&gt;

&lt;h3 id=&#34;api:441ef2dfb3e97650c90da967029aaee7&#34;&gt;API&lt;/h3&gt;

&lt;p&gt;通过 HTTP 访问 influxdb。&lt;/p&gt;

&lt;p&gt;语法上是一种类似于 SQL 的命令，官方称为 InfluxQL。&lt;/p&gt;

&lt;h4 id=&#34;创建数据库:441ef2dfb3e97650c90da967029aaee7&#34;&gt;创建数据库&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -POST http://localhost:8086/query --data-urlencode &amp;quot;q=CREATE DATABASE mydb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;插入数据:441ef2dfb3e97650c90da967029aaee7&#34;&gt;插入数据&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -i -XPOST &#39;http://localhost:8086/write?db=mydb&#39; --data-binary &#39;cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu_load_short 是 measurement，host 和 region 是 tags-key，value 是 field-key。&lt;/p&gt;

&lt;p&gt;多条数据时，用换行区分每条数据&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -i -XPOST &#39;http://localhost:8086/write?db=mydb&#39; --data-binary &#39;cpu_load_short,host=server02 value=0.67
cpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257
cpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;读取数据:441ef2dfb3e97650c90da967029aaee7&#34;&gt;读取数据&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -GET &#39;http://localhost:8086/query&#39; --data-urlencode &amp;quot;db=mydb&amp;quot; --data-urlencode &amp;quot;epoch=s&amp;quot; --data-urlencode &amp;quot;q=SELECT value FROM cpu_load_short WHERE region=&#39;us-west&#39;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时查询多条数据时，以分号分隔&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -G &#39;http://localhost:8086/query&#39; --data-urlencode &amp;quot;db=mydb&amp;quot; --data-urlencode &amp;quot;epoch=s&amp;quot; --data-urlencode &amp;quot;q=SELECT value FROM cpu_load_short WHERE region=&#39;us-west&#39;;SELECT count(value) FROM cpu_load_short WHERE region=&#39;us-west&#39;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里 &lt;code&gt;--data-urlencode &amp;quot;epoch=s&amp;quot;&lt;/code&gt; 会使返回的时间戳为 unix 时间戳格式。&lt;/p&gt;

&lt;h4 id=&#34;创建-rp:441ef2dfb3e97650c90da967029aaee7&#34;&gt;创建 RP&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CREATE RETENTION POLICY two_hours ON food_data DURATION 2h REPLICATION 1 DEFAULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里将 &lt;strong&gt;two_hours&lt;/strong&gt; 设置成了默认保存策略，存入 food_data 中的数据如果没有明确指定 RP 将会默认采用此策略，数据保存时间为 2 小时，副本数为 1。&lt;/p&gt;

&lt;h4 id=&#34;创建-cq:441ef2dfb3e97650c90da967029aaee7&#34;&gt;创建 CQ&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CREATE CONTINUOUS QUERY cq_5m ON food_data BEGIN SELECT mean(website) AS mean_website,mean(phone) AS mean_phone INTO food_data.&amp;quot;default&amp;quot;.downsampled_orders FROM orders GROUP BY time(5m) END
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里创建了一个 CQ，每个5分钟将 two_hours.orders 中的数据计算5分钟的平均值后存入 default.downsampled_orders 中，default 这个 RP 中的数据是永久保存的。&lt;/p&gt;

&lt;h4 id=&#34;where:441ef2dfb3e97650c90da967029aaee7&#34;&gt;WHERE&lt;/h4&gt;

&lt;p&gt;查询时指定查询的限制条件，例如查询最近1小时内 host_id=1 的机器的 cpu 数据。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SELECT value FROM cpu_load WHERE time &amp;gt; now() - 1h and host_id = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;group-by:441ef2dfb3e97650c90da967029aaee7&#34;&gt;GROUP BY&lt;/h4&gt;

&lt;p&gt;类似于 SQL 中的语法，可以对细粒度数据进行聚合计算，例如查询最近1小时内 host_id=1 的机器的 cpu 的数据，并且采样为每5分钟的平均值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SELECT mean(value) FROM cpu_load WHERE time &amp;gt; now() - 1h and host_id = 1 GROUP BY time(5m)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;官方推荐硬件配置:441ef2dfb3e97650c90da967029aaee7&#34;&gt;官方推荐硬件配置&lt;/h3&gt;

&lt;h4 id=&#34;单节点:441ef2dfb3e97650c90da967029aaee7&#34;&gt;单节点&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Load&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Writes per second&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Queries per second&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Unique series&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Low&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 5 thousand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 100 thousand&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Moderate&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 100 thousand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt; 1 million&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;High&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 100 thousand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 1 million&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Probably infeasible&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 500 thousand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 10 million&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Low: CPU 2-4, RAM 2-4GB, IOPS 500&lt;/li&gt;
&lt;li&gt;Moderate: CPU 4-6, RAM 8-32GB, IOPS 500-1000&lt;/li&gt;
&lt;li&gt;High: CPU CPU 8+, RAM 32GB+, IOPS 1000+&lt;/li&gt;
&lt;li&gt;Probably infeasible: 可能单机无法支持，需要集群环境&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;集群:441ef2dfb3e97650c90da967029aaee7&#34;&gt;集群&lt;/h4&gt;

&lt;p&gt;InfluxDB 从 0.12 版本开始将不再开源其 cluster 源码，而是被用做提供商业服务。&lt;/p&gt;

&lt;p&gt;如果考虑到以后的扩展，需要自己在前端做代理分片或者类似的开发工作。&lt;/p&gt;

&lt;p&gt;已知七牛是采用了 InfluxDB 作为时间序列数据的存储，自研了调度器以及高可用模块，具有横向扩展的能力。&lt;/p&gt;

&lt;h3 id=&#34;总结:441ef2dfb3e97650c90da967029aaee7&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;目前最火热的时间序列数据库项目，社区开发活跃，迭代更新较快，存储引擎经常变化，网上的一些资料都比较过时，例如最新的 TSM 存储引擎只能看到官方的文档简介，还没有详细的原理说明的文章。&lt;/p&gt;

&lt;p&gt;就单机来说，在磁盘占用、cpu使用率、读写速度方面都让人眼前一亮。如果数据量级不是非常大的情况下，单节点的 InfluxDB 就可以承载数十万每秒的写入，是一个比较合适的选择。&lt;/p&gt;

&lt;p&gt;另一方面，从 0.12 版本开始不再开源其集群代码（虽然之前的集群部分就比较烂），如果考虑到之后进行扩展的话，需要进行二次开发。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>时间序列数据库调研之OpenTSDB</title>
          <link>http://blog.fatedier.com/2016/07/04/research-of-time-series-database-opentsdb</link>
          <pubDate>Mon, 04 Jul 2016 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2016/07/04/research-of-time-series-database-opentsdb</guid>
          <description>

&lt;p&gt;Java 项目，基于 HBase（2.3版本貌似开始支持 Google BigTable 和 Cassandra） 的一个时间序列数据库，被广泛应用于监控系统中。很多大公司都在使用，社区较为活跃。&lt;/p&gt;

&lt;h3 id=&#34;测试版本:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;测试版本&lt;/h3&gt;

&lt;p&gt;hbase-1.1.5&lt;/p&gt;

&lt;p&gt;opentsdb-2.2.0&lt;/p&gt;

&lt;h3 id=&#34;单机部署:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;单机部署&lt;/h3&gt;

&lt;p&gt;单机部署可以参考我之前的一篇文章，集群部署比较复杂，这里仅使用单机进行测试。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.fatedier.com/2016/03/12/install-and-use-opentsdb/&#34;&gt;OpenTSDB部署与使用&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;概念:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;概念&lt;/h3&gt;

&lt;h4 id=&#34;数据格式:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;数据格式&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;metric: 一个可测量的单位的标称。&lt;/li&gt;
&lt;li&gt;tags: 对 metric 的具体描述。&lt;/li&gt;
&lt;li&gt;timestamp: 时间戳。&lt;/li&gt;
&lt;li&gt;value: metric 的实际测量值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;uid:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;UID&lt;/h4&gt;

&lt;p&gt;在 OpenTSDB 中，每一个 metric、tagk 或者 tagv 在创建的时候被分配一个唯一标识叫做 UID。在后续的实际存储中，实际上存储的是 UID，而不是它们原本的字符串，UID 占 3个字节（也可以修改源码改为4字节），这样可以节省存储空间。&lt;/p&gt;

&lt;h4 id=&#34;tsuid:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;TSUID&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;metric_UID&amp;gt;&amp;lt;timestamp&amp;gt;&amp;lt;tagk1_UID&amp;gt;&amp;lt;tagv1_UID&amp;gt;[...&amp;lt;tagkN_UID&amp;gt;&amp;lt;tagvN_UID&amp;gt;]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;写入 HBase 时的 row key 格式，其中的 metric、tagk 和 tagv 都被转换成了 UID。&lt;/p&gt;

&lt;h4 id=&#34;data-table-schema:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;Data Table Schema&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2016/2016-07-04-research-of-time-series-database-opentsdb-data-table-schema.png&#34; alt=&#34;data-table-schema&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RowKey&lt;/strong&gt; 就是上述的 TSUID，除了时间戳占 4 byte，其余 UID 占 3 byte。&lt;/p&gt;

&lt;p&gt;时间戳的部分只保留到了小时粒度，具体相对于小时的偏移量被存储在了 &lt;strong&gt;列族 t&lt;/strong&gt; 中。这样就减小了 HBase 中的存储行数。也就是说对于同一个小时的 metric + tags 相同的数据都会存放在一个 row 下面，这样的设计提高了 row 的检索速度。&lt;/p&gt;

&lt;p&gt;这样的 RowKey 设计使得 metric + tags 相同的数据都会连续存放，且 metric 相同的数据也会连续存放，底层 HBase 中会放在同一 Region 中，在做 Scan 的时候可以快速读取到大片数据，加速查询的过程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;value&lt;/strong&gt; 使用 8 bytes 存储，既可以存储 long,也可以存储 double。&lt;/p&gt;

&lt;h4 id=&#34;compaction:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;Compaction&lt;/h4&gt;

&lt;p&gt;在 OpenTSDB 中，会将多列合并到一列之中以减少磁盘占用空间，这个过程会在 TSD 写数据或者查询过程中不定期的发生。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2016/2016-07-04-research-of-time-series-database-opentsdb-compaction.png&#34; alt=&#34;compaction&#34; /&gt;&lt;/p&gt;

&lt;p&gt;例如图中，将列 1890 和 列 1892 合并到了一起。&lt;/p&gt;

&lt;h3 id=&#34;api:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;API&lt;/h3&gt;

&lt;p&gt;OpenTSDB 同样提供了一套基于 HTTP 的 API 接口。&lt;/p&gt;

&lt;h4 id=&#34;插入数据:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;插入数据&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:4242/api/put&#34;&gt;http://localhost:4242/api/put&lt;/a&gt;, POST&lt;/p&gt;

&lt;p&gt;内容为 JSON 格式，支持同时插入多条数据。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[
    {
        &amp;quot;metric&amp;quot;: &amp;quot;sys.cpu.nice&amp;quot;,
        &amp;quot;timestamp&amp;quot;: 1346846400,
        &amp;quot;value&amp;quot;: 18,
        &amp;quot;tags&amp;quot;: {
           &amp;quot;host&amp;quot;: &amp;quot;web01&amp;quot;,
           &amp;quot;dc&amp;quot;: &amp;quot;lga&amp;quot;
        }
    },
    {
        &amp;quot;metric&amp;quot;: &amp;quot;sys.cpu.nice&amp;quot;,
        &amp;quot;timestamp&amp;quot;: 1346846400,
        &amp;quot;value&amp;quot;: 9,
        &amp;quot;tags&amp;quot;: {
           &amp;quot;host&amp;quot;: &amp;quot;web02&amp;quot;,
           &amp;quot;dc&amp;quot;: &amp;quot;lga&amp;quot;
        }
    }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;查询数据:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;查询数据&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:4242/api/query&#34;&gt;http://localhost:4242/api/query&lt;/a&gt;, POST&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;start&amp;quot;: 1463452826,
    &amp;quot;end&amp;quot;: 1463453026,
    &amp;quot;globalAnnotations&amp;quot;: true,
    &amp;quot;queries&amp;quot;: [
        {
            &amp;quot;aggregator&amp;quot;: &amp;quot;avg&amp;quot;,
            &amp;quot;metric&amp;quot;: &amp;quot;sys.disk.usage&amp;quot;,
            &amp;quot;downsample&amp;quot;: &amp;quot;60s-avg&amp;quot;,
            &amp;quot;tags&amp;quot;: {
                &amp;quot;host_id&amp;quot;: &amp;quot;123&amp;quot;
            }
        },
        {
            &amp;quot;aggregator&amp;quot;: &amp;quot;sum&amp;quot;,
            &amp;quot;metric&amp;quot;: &amp;quot;sys.cpu.load&amp;quot;,
            &amp;quot;downsample&amp;quot;: &amp;quot;60s-avg&amp;quot;,
            &amp;quot;tags&amp;quot;: {
                &amp;quot;host_id&amp;quot;: &amp;quot;123&amp;quot;
            }
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;start&lt;/strong&gt; 和 &lt;strong&gt;end&lt;/strong&gt; 指定了查询的时间范围。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tags&lt;/strong&gt; 指定了过滤条件，2.2 版本中将不被推荐，取而代之的是 filters 参数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;downsample&lt;/strong&gt; 聚合计算，例如上面是对每隔60s的数据计算一次平均值。&lt;/p&gt;

&lt;h3 id=&#34;总结:2bcfdb374ee9d4126e3ae956caebf257&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;OpenTSDB 在存储时间序列数据这一领域拥有很大的优势，被大多数公司所采用，网上的相关文档也比较全面。&lt;/p&gt;

&lt;p&gt;底层存储依托于 HBase，采用特殊设计过的数据存储格式，提供了非常快的查询速度，在此基础之上也更容易横向扩展。&lt;/p&gt;

&lt;p&gt;但是，相对于 InfluxDB 这种即使是新手也可以在两分钟内部署运行完成，OpenTSDB 的部署和运维显然要麻烦很多，由于底层依赖于 HBase，想要大规模运行起来，需要相当专业、细心的运维工作。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>部署openstack的对象存储服务swift</title>
          <link>http://blog.fatedier.com/2016/05/25/deploy-openstack-swift</link>
          <pubDate>Wed, 25 May 2016 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2016/05/25/deploy-openstack-swift</guid>
          <description>

&lt;p&gt;OpenStack Swift 是一个开源项目，提供了弹性可伸缩、高可用的分布式对象存储服务，适合存储大规模非结构化数据。由于要开发自己的分布式存储应用，需要借鉴 swift 的一些架构，所以在自己的机器上搭建了一个集群环境用于测试。&lt;/p&gt;

&lt;h3 id=&#34;环境:15a147333eadc9d914016442056e059f&#34;&gt;环境&lt;/h3&gt;

&lt;p&gt;一台物理机上，开了三台虚拟机，操作系统是 Centos7&lt;/p&gt;

&lt;p&gt;ip 为 &lt;strong&gt;192.168.2.129, 192.168.2.130, 192.168.2.131&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;之前安装过一次，当时是 2.4.0 版本，目前最新的是 2.7.0 版本，最新的版本由于我的机器上缺少某些动态库，所以还是选择了比较熟悉的 2.4.0 版本，基本上的步骤是一样的。&lt;/p&gt;

&lt;p&gt;后续的安装步骤需要在三台机器上全部执行一遍，下文中的 &lt;code&gt;user:user&lt;/code&gt; 根据需要修改为自己机器上的用户。&lt;/p&gt;

&lt;p&gt;整个步骤参考了官方的文档： &lt;a href=&#34;http://docs.openstack.org/developer/swift/development_saio.html&#34;&gt;http://docs.openstack.org/developer/swift/development_saio.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装各种依赖:15a147333eadc9d914016442056e059f&#34;&gt;安装各种依赖&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo yum install curl gcc memcached rsync sqlite xfsprogs git-core \
    libffi-devel xinetd liberasurecode-devel \
    python-setuptools \
    python-coverage python-devel python-nose \
    pyxattr python-eventlet \
    python-greenlet python-paste-deploy \
    python-netifaces python-pip python-dns \
    python-mock
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;创建磁盘设备:15a147333eadc9d914016442056e059f&#34;&gt;创建磁盘设备&lt;/h3&gt;

&lt;p&gt;由于 swift 需要使用 xfs 格式的磁盘，这里我们使用回环设备。&lt;/p&gt;

&lt;h4 id=&#34;创建一个用于回环设备的2gb大小的文件:15a147333eadc9d914016442056e059f&#34;&gt;创建一个用于回环设备的2GB大小的文件&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mkdir /srv
sudo truncate -s 1GB /srv/swift-disk
sudo mkfs.xfs /srv/swift-disk
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;修改-etc-fstab:15a147333eadc9d914016442056e059f&#34;&gt;修改 /etc/fstab&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/srv/swift-disk /srv/node/sdb1 xfs loop,noatime,nodiratime,nobarrier,logbufs=8 0 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;创建挂载点并进行挂载:15a147333eadc9d914016442056e059f&#34;&gt;创建挂载点并进行挂载&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mkdir -p /srv/node/sdb1
sudo mount /srv/node/sdb1
sudo chown -R user:user /srv/node
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;下载安装-swift-和-swift-client:15a147333eadc9d914016442056e059f&#34;&gt;下载安装 swift 和 swift-client&lt;/h4&gt;

&lt;p&gt;从 git 下载这两个项目之后切换到 2.4.0 版本，安装 python 依赖以后使用 &lt;code&gt;python setup.py&lt;/code&gt; 安装。&lt;/p&gt;

&lt;p&gt;直接通过 yum 安装的 pip 可能版本不是最新的，需要升级，使用 &lt;code&gt;pip install --upgrade pip&lt;/code&gt; 升级。&lt;/p&gt;

&lt;h5 id=&#34;swift-client:15a147333eadc9d914016442056e059f&#34;&gt;swift-client&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/openstack/python-swiftclient.git
git checkout 2.4.0
cd ./python-swiftclient; sudo python setup.py develop; cd -
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;swift:15a147333eadc9d914016442056e059f&#34;&gt;swift&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/openstack/swift.git
git checkout 2.4.0
cd ./swift; sudo pip install -r requirements.txt; sudo python setup.py develop; cd -
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;配置-etc-rc-local:15a147333eadc9d914016442056e059f&#34;&gt;配置 /etc/rc.local&lt;/h5&gt;

&lt;p&gt;需要在每次开机时都创建一些需要的目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mkdir -p /var/run/swift
sudo chown user:user /var/run/swift
sudo mkdir -p /var/cache/swift
sudo chown user:user /var/cache/swift
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置-rsync:15a147333eadc9d914016442056e059f&#34;&gt;配置 rsync&lt;/h3&gt;

&lt;h4 id=&#34;修改-etc-rsyncd-conf:15a147333eadc9d914016442056e059f&#34;&gt;修改 /etc/rsyncd.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# /etc/rsyncd: configuration file for rsync daemon mode
uid = user
gid = user
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = 0.0.0.0

[account]
max connections = 25
path = /srv/node/
read only = false
lock file = /var/lock/account.lock

[container]
max connections = 25
path = /srv/node/
read only = false
lock file = /var/lock/container.lock

[object]
max connections = 25
path = /srv/node/
read only = false
lock file = /var/lock/object.lock
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;启动-rsync-并设置开机启动:15a147333eadc9d914016442056e059f&#34;&gt;启动 rsync 并设置开机启动&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo chkconfig rsyncd on
sudo service rsyncd start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置rsyslog让每个程序输出独立的日志-可选:15a147333eadc9d914016442056e059f&#34;&gt;配置rsyslog让每个程序输出独立的日志（可选）&lt;/h3&gt;

&lt;p&gt;swift 默认将日志信息输出到文件 /var/log/syslog 中。如果要按照个人需求设置 rsyslog，生成另外单独的 swift日志文件，就需要另外配置。&lt;/p&gt;

&lt;h4 id=&#34;创建日志配置文件-etc-rsyslog-d-10-swift-conf:15a147333eadc9d914016442056e059f&#34;&gt;创建日志配置文件 /etc/rsyslog.d/10-swift.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Uncomment the following to have a log containing all logs together
#local1,local2,local3,local4,local5.*   /var/log/swift/all.log

# Uncomment the following to have hourly proxy logs for stats processing
$template HourlyProxyLog,&amp;quot;/var/log/swift/hourly/%$YEAR%%$MONTH%%$DAY%%$HOUR%&amp;quot;
#local1.*;local1.!notice ?HourlyProxyLog

local1.*;local1.!notice /var/log/swift/proxy.log
local1.notice           /var/log/swift/ proxy.error
local1.*                ~

local2.*;local2.!notice /var/log/swift/object.log
local2.notice           /var/log/swift/ object.error
local2.*                ~

local3.*;local3.!notice /var/log/swift/container.log
local3.notice           /var/log/swift/ container.error
local3.*                ~

local4.*;local4.!notice /var/log/swift/account.log
local4.notice           /var/log/swift/ account.error
local4.*                ~
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;编辑文件-etc-rsyslog-conf-更改参数-privdroptogroup-为-adm:15a147333eadc9d914016442056e059f&#34;&gt;编辑文件 /etc/rsyslog.conf，更改参数 $PrivDropToGroup 为 adm&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$PrivDropToGroup adm
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;创建-var-log-swift-目录-用于存放独立日志:15a147333eadc9d914016442056e059f&#34;&gt;创建 /var/log/swift 目录，用于存放独立日志&lt;/h4&gt;

&lt;p&gt;此外，上面的 &lt;code&gt;10-swift.conf&lt;/code&gt; 文件中设置了输出 Swift Proxy Server 每小时的 stats 日志信息，于是也要创建 &lt;code&gt;/var/log/swift/hourly&lt;/code&gt; 目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mkdir -p /var/log/swift/hourly
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;更改-swift-独立日志目录的权限:15a147333eadc9d914016442056e059f&#34;&gt;更改 swift 独立日志目录的权限&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chown -R root.adm /var/log/swift
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;重启-rsyslog-服务:15a147333eadc9d914016442056e059f&#34;&gt;重启 rsyslog 服务&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo service rsyslog restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;启动-memcached-并设置开机启动:15a147333eadc9d914016442056e059f&#34;&gt;启动 memcached 并设置开机启动&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo service memcached start
sudo chkconfig memcached on
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置swift相关的配置文件:15a147333eadc9d914016442056e059f&#34;&gt;配置swift相关的配置文件&lt;/h3&gt;

&lt;h4 id=&#34;创建存储-swift-配置文件的目录:15a147333eadc9d914016442056e059f&#34;&gt;创建存储 swift 配置文件的目录&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo mkdir /etc/swift
sudo chown -R user:user /etc/swift
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;创建-swift-配置文件-etc-swift-swift-conf:15a147333eadc9d914016442056e059f&#34;&gt;创建 swift 配置文件 /etc/swift/swift.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[swift-hash]

# random unique string that can never change (DO NOT LOSE)
swift_hash_path_suffix = jtangfs
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;配置-account-服务-etc-swift-account-server-conf:15a147333eadc9d914016442056e059f&#34;&gt;配置 account 服务 /etc/swift/account-server.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DEFAULT]
devices = /srv/node
mount_check = false
bind_ip = 0.0.0.0
bind_port = 6002
workers = 4
user = wcl
log_facility = LOG_LOCAL4

[pipeline:main]
pipeline = account-server

[app:account-server]
use = egg:swift#account

[account-replicator]
[account-auditor]
[account-reaper]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;配置-container-服务-etc-swift-container-server-conf:15a147333eadc9d914016442056e059f&#34;&gt;配置 container 服务 /etc/swift/container-server.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DEFAULT]
devices = /srv/node
mount_check = false
bind_ip = 0.0.0.0
bind_port = 6001
workers = 4
user = wcl
log_facility = LOG_LOCAL3

[pipeline:main]
pipeline = container-server

[app:container-server]
use = egg:swift#container

[container-replicator]

[container-updater]

[container-auditor]

[container-sync]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;配置-object-服务-etc-swift-object-server-conf:15a147333eadc9d914016442056e059f&#34;&gt;配置 object 服务 /etc/swift/object-server.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DEFAULT]
devices = /srv/node
mount_check = false
bind_ip = 0.0.0.0
bind_port = 6000
workers = 4
user = wcl
log_facility = LOG_LOCAL2

[pipeline:main]
pipeline = object-server

[app:object-server]
use = egg:swift#object

[object-replicator]

[object-updater]

[object-auditor]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;配置-proxy-服务-etc-swift-proxy-server-conf:15a147333eadc9d914016442056e059f&#34;&gt;配置 proxy 服务 /etc/swift/proxy-server.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DEFAULT]
bind_port = 8080
user = wcl
workers = 2
log_facility = LOG_LOCAL1

[pipeline:main]
pipeline = healthcheck cache tempauth proxy-logging proxy-server

[app:proxy-server]
use = egg:swift#proxy
allow_account_management = true
account_autocreate = true

[filter:tempauth]
use = egg:swift#tempauth
user_admin_admin = admin .admin .reseller_admin
user_test_tester = testing .admin
user_test2_tester2 = testing2 .admin
user_test_tester3 = testing3
reseller_prefix = AUTH
token_life = 86400
# account和token的命名前缀，注意此处不可以加&amp;quot;_&amp;quot;。
# 例如X-Storage-Ur可能l为http://127.0.0.1:8080/v1/AUTH_test
# 例如X-Auth-Token为AUTH_tk440e9bd9a9cb46d6be07a5b6a585f7d1# token的有效期，单位：秒。

[filter:healthcheck]
use = egg:swift#healthcheck

[filter:cache]
use = egg:swift#memcache

# 这里可以是多个memcache server，proxy会自动当作一个集群来使用# 例如 memcache_servers = 192.168.2.129:11211,192.168.2.130:11211,192.168.2.131

[filter:proxy-logging]
use = egg:swift#proxy_logging
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注：&lt;code&gt;user_admin_admin = admin .admin .reseller_admin&lt;/code&gt; 后面还可以增加一项 &lt;code&gt;&amp;lt;storage-url&amp;gt;&lt;/code&gt;，显示地指定 swift 为该用户提供的存储服务入口，admin 用户通过认证后，swift 会把 Token 和该 &lt;code&gt;&amp;lt;storage-url&amp;gt;&lt;/code&gt;  返回给用户，此后 admin 用户可以使用该 &lt;code&gt;&amp;lt;storage-url&amp;gt;&lt;/code&gt; 访问 swift 来请求存储服务。特别值得说明的是，如果在 Proxy Server 前面增加了负载均衡器（如 nginx），那么该 &lt;code&gt;&amp;lt;storage-url&amp;gt;&lt;/code&gt; 应该指向负载均衡器，使得用户在通过认证后，向负载均衡器发起存储请求，再由负载均衡器将请求均衡地分发给 Proxy Server 集群。此时的 &lt;code&gt;&amp;lt;storage-url&amp;gt;&lt;/code&gt; 形如 &lt;code&gt;http://&amp;lt;LOAD_BALANCER_HOSTNAME&amp;gt;:&amp;lt;PORT&amp;gt;/v1/AUTH_admin&lt;/code&gt;。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建-ring-文件:15a147333eadc9d914016442056e059f&#34;&gt;创建 ring 文件&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;swift&lt;/strong&gt; 安装完成后会有一个 &lt;strong&gt;swift-ring-builder&lt;/strong&gt; 程序用于对 ring 的相关操作。&lt;/p&gt;

&lt;h4 id=&#34;生成-ring:15a147333eadc9d914016442056e059f&#34;&gt;生成 ring&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift-ring-builder account.builder create 18 3 1
swift-ring-builder container.builder create 18 3 1
swift-ring-builder object.builder create 18 3 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要事先创建好3个 ring，18 表示 partition 数为2的18次方，3表示 replication 数为3，1 表示分区数据的最短迁移间隔时间为1小时。（官网说明里如果移除设备的话不在这个限制内）&lt;/p&gt;

&lt;h3 id=&#34;向-ring-中加入设备:15a147333eadc9d914016442056e059f&#34;&gt;向 ring 中加入设备&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift-ring-builder account.builder add z1-192.168.2.129:6002/sdb1 100
swift-ring-builder container.builder add z1-192.168.2.129:6001/sdb1 100
swift-ring-builder object.builder add z1-192.168.2.129:6000/sdb1 100

swift-ring-builder account.builder add z2-192.168.2.130:6002/sdb1 100
swift-ring-builder container.builder add z2-192.168.2.130:6001/sdb1 100
swift-ring-builder object.builder add z2-192.168.2.130:6000/sdb1 100

swift-ring-builder account.builder add z2-192.168.2.131:6002/sdb1 100
swift-ring-builder container.builder add z2-192.168.2.131:6001/sdb1 100
swift-ring-builder object.builder add z2-192.168.2.131:6000/sdb1 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;z1&lt;/strong&gt; 和 &lt;strong&gt;z2&lt;/strong&gt; 表示 &lt;strong&gt;zone1&lt;/strong&gt; 和 &lt;strong&gt;zone2&lt;/strong&gt;（&lt;strong&gt;zone&lt;/strong&gt; 这个概念是虚拟的，可以将一个 &lt;strong&gt;device&lt;/strong&gt; 划定到一个 &lt;strong&gt;zone&lt;/strong&gt;，在分配 &lt;strong&gt;partition&lt;/strong&gt; 的时候会考虑到这个因素，尽量划分到不同的 &lt;strong&gt;zone&lt;/strong&gt; 中）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;sdb1&lt;/strong&gt; 为 &lt;strong&gt;swift&lt;/strong&gt; 所使用的存储空间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;100&lt;/strong&gt; 代表设备的权重，也是在分配 &lt;strong&gt;partition&lt;/strong&gt; 的时候会考虑的因素。&lt;/p&gt;

&lt;h4 id=&#34;rebalance:15a147333eadc9d914016442056e059f&#34;&gt;rebalance&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift-ring-builder account.builder rebalance
swift-ring-builder container.builder rebalance
swift-ring-builder object.builder rebalance
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;将-ring-文件传输到其他机器的-etc-swift-目录下:15a147333eadc9d914016442056e059f&#34;&gt;将 ring 文件传输到其他机器的 /etc/swift 目录下&lt;/h4&gt;

&lt;p&gt;最终生成的 ring 文件以 &lt;code&gt;.gz&lt;/code&gt; 结尾。&lt;/p&gt;

&lt;h3 id=&#34;创建初始化-起停等脚本:15a147333eadc9d914016442056e059f&#34;&gt;创建初始化、起停等脚本&lt;/h3&gt;

&lt;p&gt;由于 swift 涉及的组件较多，起停都比较麻烦，所以最好先写好起停脚本。&lt;/p&gt;

&lt;h4 id=&#34;remake-rings-sh-脚本文件-用于完成-ring-的重新创建:15a147333eadc9d914016442056e059f&#34;&gt;remake_rings.sh 脚本文件，用于完成 ring 的重新创建&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
cd /etc/swift

rm -f *.builder *.ring.gz backups/*.builder backups/*.ring.gz

swift-ring-builder account.builder create 18 3 1
swift-ring-builder container.builder create 18 3 1
swift-ring-builder object.builder create 18 3 1

swift-ring-builder account.builder add z1-192.168.2.129:6002/sdb1 100
swift-ring-builder container.builder add z1-192.168.2.129:6001/sdb1 100
swift-ring-builder object.builder add z1-192.168.2.129:6000/sdb1 100

swift-ring-builder account.builder add z1-192.168.2.130:6002/sdb1 100
swift-ring-builder container.builder add z1-192.168.2.130:6001/sdb1 100
swift-ring-builder object.builder add z1-192.168.2.130:6000/sdb1 100

swift-ring-builder account.builder add z1-192.168.2.131:6002/sdb1 100
swift-ring-builder container.builder add z1-192.168.2.131:6001/sdb1 100
swift-ring-builder object.builder add z1-192.168.2.131:6000/sdb1 100

swift-ring-builder account.builder rebalance
swift-ring-builder container.builder rebalance
swift-ring-builder object.builder rebalance
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;reset-swift-sh-脚本文件-用于一键清空-swift-的对象数据和日志-完全重置:15a147333eadc9d914016442056e059f&#34;&gt;reset_swift.sh 脚本文件，用于一键清空 swift 的对象数据和日志，完全重置&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
swift-init all stop

find /var/log/swift -type f -exec rm -f {} \;

sudo umount /srv/node/sdb1
sudo mkfs.xfs -f -i size=1024 /srv/swift-disk
sudo mount /srv/node/sdb1

sudo chown wcl:wcl /srv/node/sdb1

sudo rm -f /var/log/debug /var/log/messages /var/log/rsyncd.log /var/log/syslog
sudo service rsyslog restart
sudo service rsyncd restart
sudo service memcached restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;start-main-sh-脚本文件-用于启动-swift-所有基础服务:15a147333eadc9d914016442056e059f&#34;&gt;start_main.sh 脚本文件，用于启动 swift 所有基础服务&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
swift-init main start
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;stop-main-sh-脚本文件-用于关闭-swift-所有基础服务:15a147333eadc9d914016442056e059f&#34;&gt;stop_main.sh 脚本文件，用于关闭 swift 所有基础服务&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
swift-init main stop
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;start-all-sh-脚本文件:15a147333eadc9d914016442056e059f&#34;&gt;start_all.sh 脚本文件&lt;/h4&gt;

&lt;p&gt;一键启动 swift 的所有服务。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
swift-init proxy start
swift-init account-server start
swift-init account-replicator start
swift-init account-auditor start
swift-init container-server start
swift-init container-replicator start
swift-init container-updater start
swift-init container-auditor start
swift-init object-server start
swift-init object-replicator start
swift-init object-updater start
swift-init object-auditor start
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;stop-all-sh-脚本文件:15a147333eadc9d914016442056e059f&#34;&gt;stop_all.sh 脚本文件&lt;/h4&gt;

&lt;p&gt;一键关闭 swift的所有服务。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
swift-init proxy stop
swift-init account-server stop
swift-init account-replicator stop
swift-init account-auditor stop
swift-init container-server stop
swift-init container-replicator stop
swift-init container-updater stop
swift-init container-auditor stop
swift-init object-server stop
swift-init object-replicator stop
swift-init object-updater stop
swift-init object-auditor stop
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;启动-swift-服务:15a147333eadc9d914016442056e059f&#34;&gt;启动 swift 服务&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./start_all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;动态扩容:15a147333eadc9d914016442056e059f&#34;&gt;动态扩容&lt;/h3&gt;

&lt;p&gt;重新编写 &lt;strong&gt;remake_rings.sh&lt;/strong&gt; 脚本中的内容，加上需要添加的机器信息，重新生成 ring 文件。&lt;/p&gt;

&lt;p&gt;将新生成的 ring 文件放到每一个服务器的 &lt;code&gt;/etc/swift&lt;/code&gt; 目录下。&lt;/p&gt;

&lt;p&gt;swift 的各个组件程序会定期重新读取 rings 文件。&lt;/p&gt;

&lt;h3 id=&#34;一些需要注意的地方:15a147333eadc9d914016442056e059f&#34;&gt;一些需要注意的地方&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;服务器时间必须一致，不一致的话会出现问题。测试中有一台服务器比其他的慢了2小时，结果向这台 proxy  上传文件，返回成功，但是其实并没有上传成功，可能是由于时间原因，在其他机器看来这个文件已经被删除掉了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;restful-api-及测试:15a147333eadc9d914016442056e059f&#34;&gt;RESTful API 及测试&lt;/h3&gt;

&lt;p&gt;swift 可以通过 swift-proxy 提供的 RESTful API 来进行操作。&lt;/p&gt;

&lt;p&gt;另外一种方法就是使用 &lt;code&gt;swift&lt;/code&gt; 程序来进行增删查改的操作，可以实现和 RESTful API 相同的功能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;所有的操作都需要先获取一个 auth-token，之后的所有操作都需要在 header 中附加上 X-Auth-Token 字段的信息进行权限认证。有一定时效性，过期后需要再次获取。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;获取-x-storage-url-和-x-auth-token:15a147333eadc9d914016442056e059f&#34;&gt;获取 X-Storage-Url 和 X-Auth-Token&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://127.0.0.1:8080/auth/v1.0 -v -H &#39;X-Storage-User: test:tester&#39; -H &#39;X-Storage-Pass: testing&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的用户和密码是在 &lt;code&gt;proxy-server.conf&lt;/code&gt; 中配置的。&lt;/p&gt;

&lt;p&gt;返回结果&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;lt; HTTP/1.1 200 OK
&amp;lt; X-Storage-Url: http://127.0.0.1:8080/v1/AUTH_test
&amp;lt; X-Auth-Token: AUTH_tk039a65cb62594b319faea9dc492039a2
&amp;lt; Content-Type: text/html; charset=UTF-8
&amp;lt; X-Storage-Token: AUTH_tk039a65cb62594b319faea9dc492039a2
&amp;lt; X-Trans-Id: tx7bcd450378d84b56b1482-005745b5c8
&amp;lt; Content-Length: 0
&amp;lt; Date: Wed, 25 May 2016 14:25:12 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;查看账户信息:15a147333eadc9d914016442056e059f&#34;&gt;查看账户信息&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://127.0.0.1:8080/v1/AUTH_test -v -H &#39;X-Auth-Token: AUTH_tk039a65cb62594b319faea9dc492039a2&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回结果&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;lt; HTTP/1.1 200 OK
&amp;lt; Content-Length: 5
&amp;lt; X-Account-Object-Count: 1
&amp;lt; X-Account-Storage-Policy-Policy-0-Bytes-Used: 4
&amp;lt; X-Account-Storage-Policy-Policy-0-Container-Count: 1
&amp;lt; X-Timestamp: 1464106581.68979
&amp;lt; X-Account-Storage-Policy-Policy-0-Object-Count: 1
&amp;lt; X-Account-Bytes-Used: 4
&amp;lt; X-Account-Container-Count: 1
&amp;lt; Content-Type: text/plain; charset=utf-8
&amp;lt; Accept-Ranges: bytes
&amp;lt; X-Trans-Id: tx59e9c2f1772349d684d04-005745b713
&amp;lt; Date: Wed, 25 May 2016 14:30:43 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;创建-container:15a147333eadc9d914016442056e059f&#34;&gt;创建 container&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://127.0.0.1:8080/v1/AUTH_test/default -X PUT -H &amp;quot;X-Auth_Token: AUTH_tk039a65cb62594b319faea9dc492039a2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 test 用户下创建了一个名为 default 的 container。&lt;/p&gt;

&lt;h4 id=&#34;restful-api-总结:15a147333eadc9d914016442056e059f&#34;&gt;RESTful API 总结&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;资源类型&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;URL&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;GET&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;PUT&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;POST&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;DELETE&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;HEAD&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;账户&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/account/&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取容器列表&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取账户元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;容器&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/account/container&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取对象列表&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;创建容器&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;更新容器元数据&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;删除容器&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取容器元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;对象&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;/account/container/object&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取对象内容和元数据&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;创建、更新或拷贝对象&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;更新对象元数据&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;删除对象&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;获取对象元数据&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;直接使用-swift-客户端程序进行操作:15a147333eadc9d914016442056e059f&#34;&gt;直接使用 swift 客户端程序进行操作&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;-A&lt;/strong&gt; 参数指定 url，&lt;strong&gt;-U&lt;/strong&gt; 参数指定用户，&lt;strong&gt;-K&lt;/strong&gt; 参数指定密码。&lt;/p&gt;

&lt;h5 id=&#34;查看指定账户的-swift-存储信息:15a147333eadc9d914016442056e059f&#34;&gt;查看指定账户的 swift 存储信息&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing stat
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;创建-container-1:15a147333eadc9d914016442056e059f&#34;&gt;创建 container&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing post default
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;查看-test-用户的-container-列表:15a147333eadc9d914016442056e059f&#34;&gt;查看 test 用户的 container 列表&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing list
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;上传object-文件-上传-test-cpp-文件到-default-容器中:15a147333eadc9d914016442056e059f&#34;&gt;上传Object（文件），上传 test.cpp 文件到 default 容器中&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing upload default ./dd.cpp
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;查看容器中的内容:15a147333eadc9d914016442056e059f&#34;&gt;查看容器中的内容&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing list default
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;下载object-文件:15a147333eadc9d914016442056e059f&#34;&gt;下载Object（文件）&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;swift -A http://127.0.0.1:8080/auth/v1.0 -U test:tester -K testing download default dd.cpp
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>codis 2.x版本环境搭建与测试</title>
          <link>http://blog.fatedier.com/2015/10/07/installation-and-testing-of-codis-version-two</link>
          <pubDate>Wed, 07 Oct 2015 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2015/10/07/installation-and-testing-of-codis-version-two</guid>
          <description>

&lt;p&gt;Codis 是一个分布式 Redis 解决方案, 对于上层的应用来说, 连接到 Codis Proxy 和连接原生的 Redis Server 没有明显的区别（有一些命令不支持），上层应用可以像使用单机的 Redis 一样，Codis 底层会处理请求的转发。Codis 支持不停机进行数据迁移, 对于前面的客户端来说是透明的, 可以简单的认为后面连接的是一个内存无限大的 Redis 服务。&lt;/p&gt;

&lt;h3 id=&#34;安装并启动-zookeeper:57de784f564f909e53fd43ba72fc074a&#34;&gt;安装并启动 zookeeper&lt;/h3&gt;

&lt;p&gt;codis 2.x 版本重度依赖于 zookeeper。&lt;/p&gt;

&lt;p&gt;从官网下载 &lt;a href=&#34;http://zookeeper.apache.org/releases.html&#34;&gt;zookeeper&lt;/a&gt;，解压安装。&lt;/p&gt;

&lt;p&gt;使用默认配置启动 zookeeper &lt;code&gt;sh ./bin/zkServer.sh start&lt;/code&gt;，监听地址为 &lt;code&gt;2181&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;下载安装-codis:57de784f564f909e53fd43ba72fc074a&#34;&gt;下载安装 codis&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;go get -d github.com/CodisLabs/codis&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;进入源码根目录 &lt;code&gt;cd $GOPATH/src/github.com/CodisLabs/codis&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;执行安装脚本 &lt;code&gt;./bootstrap.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注：这里第一步和第三步（会下载第三方库到本地）需要从 github copy 数据，鉴于网络环境的问题，时间会比较长。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;之后生成的可执行文件都在 &lt;code&gt;codis/bin&lt;/code&gt; 文件夹下。&lt;/p&gt;

&lt;h3 id=&#34;部署-codis-server:57de784f564f909e53fd43ba72fc074a&#34;&gt;部署 codis-server&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;codis-server&lt;/strong&gt; 基于 &lt;strong&gt;redis 2.8.21&lt;/strong&gt; 稍微进行了一些修改以支持原子性的迁移数据，具体用法和 redis 一致。&lt;/p&gt;

&lt;p&gt;将 &lt;code&gt;bin&lt;/code&gt; 文件夹下的 codis-server 拷贝到集群中每个节点，和 redis-server 的启动命令一样，指定配置文件，启动。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这里要注意 redis.conf 配置中需要设置 maxmemory，不然无法自动按照负载均衡的方式分配 slot（可以手动分配），推荐单台机器部署多个 redis 实例。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-server ./redis_conf/redis_6400.conf&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-dashboard:57de784f564f909e53fd43ba72fc074a&#34;&gt;启动 dashboard&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;dashboard&lt;/strong&gt; 既是 codis 集群的管理中心，又提供了一个人性化的 web 界面，方便查看统计信息以及对集群进行管理操作。&lt;/p&gt;

&lt;p&gt;启动 web 控制面板，注意这里要用到配置文件，不指定的话就是当前目录下的 config.ini，可以用 &lt;code&gt;-c&lt;/code&gt; 参数指定。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nohup ./bin/codis-config -c ./config.ini dashboard --addr=:18087 &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;初始化-slot:57de784f564f909e53fd43ba72fc074a&#34;&gt;初始化 slot&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot init&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;该命令会在 zookeeper 上创建 slot 相关信息。&lt;/p&gt;

&lt;h3 id=&#34;添加-group:57de784f564f909e53fd43ba72fc074a&#34;&gt;添加 group&lt;/h3&gt;

&lt;p&gt;每个 &lt;strong&gt;group&lt;/strong&gt; 只能有一个 &lt;strong&gt;master&lt;/strong&gt; 和多个 &lt;strong&gt;slave&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;命令格式： &lt;code&gt;codis-config -c ./config.ini server add &amp;lt;group_id&amp;gt; &amp;lt;redis_addr&amp;gt; &amp;lt;role&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;例如向 group 1 和 group 2 中各加入两个 codis-server 实例，一主一从。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 1 localhost:6379 master&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 1 localhost:6380 slave&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 2 localhost:6381 master&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini server add 2 localhost:6382 slave&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 代表 group_id，必须为数字，且从 1 开始&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;分配-slot:57de784f564f909e53fd43ba72fc074a&#34;&gt;分配 slot&lt;/h3&gt;

&lt;h4 id=&#34;手动分配:57de784f564f909e53fd43ba72fc074a&#34;&gt;手动分配&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;codis-config -c ./config.ini slot range-set &amp;lt;slot_from&amp;gt; &amp;lt;slot_to&amp;gt; &amp;lt;group_id&amp;gt; &amp;lt;status&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;slot&lt;/strong&gt; 默认为 &lt;strong&gt;1024&lt;/strong&gt; 个，范围是 &lt;strong&gt;0 - 1023&lt;/strong&gt;，需要将这 1024 个 slot 分配到集群中不同的 group 中。&lt;/p&gt;

&lt;p&gt;例如将 1024 个 slot 平均分配到&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot range-set 0 511 1 online&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot range-set 512 1023 2 online&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;自动分配:57de784f564f909e53fd43ba72fc074a&#34;&gt;自动分配&lt;/h4&gt;

&lt;p&gt;在 dashboard 上可以自动分配 slot，会按照负载均衡的方式进行分配，不推荐使用，因为可能会造成大量数据的迁移。&lt;/p&gt;

&lt;p&gt;或者使用命令进行自动分配&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-config -c ./config.ini slot rebalance&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-codis-proxy:57de784f564f909e53fd43ba72fc074a&#34;&gt;启动 codis-proxy&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-proxy -c ./config.ini -L ./log/proxy.log --cpu=8 --addr=10.10.100.1:19000 --http-addr=10.10.100.1:11000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：这里 &amp;ndash;addr 和 &amp;ndash;http-addr 不要填 0.0.0.0，要绑定一个具体的 ip，不然 zookeeper 中存的将是hostname，会导致 dashboard 无法连接。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;codis-proxy 是无状态的，可以部署多个，且用 go 编写，可以利用多核，建议 cpu 设置核心数的一半到2/3，19000 即为访问 redis 集群的端口，11000 为获取 proxy 相关状态的端口。&lt;/p&gt;

&lt;p&gt;之后使用 codis-config 将 codis-proxy 加入进来，也就是设置online（后来更新了一个版本，默认启动后即自动注册为online）&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bin/codis-config -c ./config.ini proxy online &amp;lt;proxy_name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;需要注意的是，启动 codis-proxy，会在 zookeeper 中注册一个 node，地址为 /zk/codis/db_test/fence，如果使用 kill -9 强行杀掉进程的话，这个会一直存在，需要手工删除。且 node 名称为 [hostname:port]，所以需要注意这个组合不能重复。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;主从切换:57de784f564f909e53fd43ba72fc074a&#34;&gt;主从切换&lt;/h3&gt;

&lt;p&gt;官方建议是手工操作，避免数据不一致的问题，但是没有自动容灾的话可用性太差。&lt;/p&gt;

&lt;p&gt;官方另外提供了一个工具，&lt;strong&gt;codis-ha&lt;/strong&gt;，这是一个通过 codis 开放的 api 实现自动切换主从的工具。该工具会在检测到 master 挂掉的时候将其下线并选择其中一个 slave 提升为 master 继续提供服务。&lt;/p&gt;

&lt;p&gt;这个工具不是很好用，如果 codis-ha 连接 dashboard 失败之后进程就会自动退出，需要手动重启或者使用 supervisor 拉起来。另外，当有机器被提升为 master 之后，其他 slave 的状态不会改变，还是从原 master 同步数据。原来的 master 重启之后处于 offline 状态，也需要手动加入 group 指定为 slave。也就是说有master 挂掉后，其余机器的状态需要手动修改。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./bin/codis-ha --codis-config=10.10.100.3:18087 --productName=common&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;10.10.100.14:18088&lt;/code&gt; 为 dashboard 所在机器的 ip 和端口。&lt;/p&gt;

&lt;h3 id=&#34;旧数据的迁移:57de784f564f909e53fd43ba72fc074a&#34;&gt;旧数据的迁移&lt;/h3&gt;

&lt;p&gt;官方提供了一个 &lt;strong&gt;redis-port&lt;/strong&gt; 工具可以将旧 redis 中的数据实时迁移到 codis 集群中，之后需要修改各服务配置文件，重启服务，指向 codis 集群即可。&lt;/p&gt;

&lt;h3 id=&#34;性能测试:57de784f564f909e53fd43ba72fc074a&#34;&gt;性能测试&lt;/h3&gt;

&lt;p&gt;测试环境： 24核 2.1GHz，4个redis实例&lt;/p&gt;

&lt;h4 id=&#34;不启用-pipeline:57de784f564f909e53fd43ba72fc074a&#34;&gt;不启用 pipeline&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;GET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MSET&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;redis单机&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58997.05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58651.02&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33557.05&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis1核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42973.79&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33003.30&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12295.58&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis4核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44208.66&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39936.10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21743.86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39478.88&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;23052.10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24679.17&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis12核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28943.56&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24224.81&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21376.66&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核2proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62085.65&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;68964.40&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48298.74&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;pipeline-100:57de784f564f909e53fd43ba72fc074a&#34;&gt;pipeline = 100&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;GET&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MSET&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;redis单机&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;259067.36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;340136.06&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40387.72&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis1核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;158982.52&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;166112.95&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15199.88&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis4核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;491159.12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;403551.25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40157.42&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;518134.72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;537634.38&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58156.44&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis12核1proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;520833.34&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500000.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53418.80&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;codis8核2proxy&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;529812.81&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;607041.47&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62872.28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过测试可以看出，使用 codis 会在性能上比原来直接使用 redis 会有所下降，但是优势就在于可以通过横向扩展（加机器）的方式去提高 redis 的存储容量以及并发量。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Redis集群调研</title>
          <link>http://blog.fatedier.com/2015/09/15/redis-cluster-survey</link>
          <pubDate>Tue, 15 Sep 2015 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://blog.fatedier.com/2015/09/15/redis-cluster-survey</guid>
          <description>

&lt;p&gt;Redis作为一个使用场景很高的NoSQL数据库，支持了较为丰富的数据类型，相比于其他关系型数据库在性能方面优势明显。互联网公司通常更加倾向于将一些热点数据放入Redis中来承载高吞吐量的访问。&lt;/p&gt;

&lt;p&gt;单机Redis在普通的服务器上通常ops上限在5w左右，开启pipeline的情况下在20-30w左右。对于大多数中小公司来说，通常单机的Redis已经足够，最多根据不同业务分散到多台Redis。&lt;/p&gt;

&lt;h3 id=&#34;为什么需要集群:51fa3b005f880190963d473247382d62&#34;&gt;为什么需要集群&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Redis单线程特性，多请求顺序执行，单个耗时的操作会阻塞后续的操作&lt;/li&gt;
&lt;li&gt;单机内存有限&lt;/li&gt;
&lt;li&gt;某些特殊业务，带宽压力较大&lt;/li&gt;
&lt;li&gt;单点问题，缺乏高可用性&lt;/li&gt;
&lt;li&gt;不能动态扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Redis集群的目标就是为了实现高可用性，避免性能瓶颈，可动态扩容，易于做监控告警。&lt;/p&gt;

&lt;h3 id=&#34;三种主流的集群解决方案:51fa3b005f880190963d473247382d62&#34;&gt;三种主流的集群解决方案&lt;/h3&gt;

&lt;h4 id=&#34;客户端静态分片:51fa3b005f880190963d473247382d62&#34;&gt;客户端静态分片&lt;/h4&gt;

&lt;p&gt;通常需要 smart-client 支持，在业务程序端根据预先设置的路由规则进行分片，从而实现对多个redis实例的分布式访问。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-jedis.png&#34; alt=&#34;jedis&#34; /&gt;&lt;/p&gt;

&lt;p&gt;鉴于redis本身的高性能，并且有一些设计良好的第三方库，例如java开发者可以使用jedis，所以很多小公司使用此方案。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt; 相比于使用代理，减少了一层网络传输的消耗，效率较高。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt; 当redis实例需要扩容或切换的情况下，需要修改业务端的程序，较为麻烦。并且需  要维护各个语言的客户端版本，如果要升级客户端成本也会比较高。出现故障时难以及时定位问题。（有些smart-client借助于zookeeper维护客户端访问redis实例的一致性）&lt;/p&gt;

&lt;h4 id=&#34;proxy分片:51fa3b005f880190963d473247382d62&#34;&gt;Proxy分片&lt;/h4&gt;

&lt;p&gt;通过统一的代理程序访问多个redis实例，比如业内曾广泛使用的 twemproxy 以及 豌豆荚开源的 codis。（twemproxy是twitter开源的一个redis和memcache代理服务器，只用于作为简单的代理中间件，目前twitter内部已经不再使用）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt; 业务程序端只需要使用普通的api去访问代理程序即可。各种组件分离，以后升级较为容易。也避免了客户端需要维持和每个redis实例的长连接导致连接数过多。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt; 增加了一层中间件，增加了网络和数据处理的消耗，性能下降。&lt;/p&gt;

&lt;h4 id=&#34;official-redis-cluster:51fa3b005f880190963d473247382d62&#34;&gt;Official Redis Cluster&lt;/h4&gt;

&lt;p&gt;Redis3.0之后的版本开始正式支持 redis cluster，核心目标是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;性能：&lt;/strong&gt;Redis作者比较看重性能，增加集群不能对性能有较大影响，所以Redis采用了P2P而非Proxy方式、异步复制、客户端重定向等设计，而牺牲了部分的一致性、使用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;水平扩展：&lt;/strong&gt;官方文档中称目标是能线性扩展到1000结点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可用性：&lt;/strong&gt;集群具有了以前Sentinel的监控和自动Failover能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;基于twemproxy的redis集群环境:51fa3b005f880190963d473247382d62&#34;&gt;基于twemproxy的redis集群环境&lt;/h3&gt;

&lt;h4 id=&#34;整体架构图:51fa3b005f880190963d473247382d62&#34;&gt;整体架构图&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-twemproxy-architecture.png&#34; alt=&#34;twemproxy_architecture&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;twemproxy的特点:51fa3b005f880190963d473247382d62&#34;&gt;twemproxy的特点&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;支持失败的节点自动摘除（仅作为缓存时）&lt;/li&gt;
&lt;li&gt;所有的key通过一致性哈希算法分布到集群中所有的redis实例中&lt;/li&gt;
&lt;li&gt;代理与每个redis实例维持长连接，减少客户端和redis实例的连接数&lt;/li&gt;
&lt;li&gt;代理是无状态的，可以任意部署多套，避免单点问题&lt;/li&gt;
&lt;li&gt;默认启用pipeline，连接复用，提高效率，性能损失在 10% - 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;集群组件:51fa3b005f880190963d473247382d62&#34;&gt;集群组件&lt;/h4&gt;

&lt;p&gt;由于twemproxy本身只是简单的代理，所以需要依赖于一些其他的程序组件。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Redis Sentinel：&lt;/strong&gt; 管理主从备份，用于主从切换，当主服务器挂掉后，自动将从服务器提升为主服务器&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Redis-Twemproxy Agent：&lt;/strong&gt; nodejs写的一个监控程序，用于监听 redis-sentinel 的 master 切换事件，并且及时更新twemproxy的配置文件后将其重新启动&lt;/p&gt;

&lt;h4 id=&#34;why-not-twemproxy:51fa3b005f880190963d473247382d62&#34;&gt;Why not Twemproxy&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;虽然使用 c 开发，性能损失较小，但同样是单线程。所以基本上twemproxy的数量需要和后端redis实例一样甚至更多才能充分发挥多实例的并发能力，造成运维困难。&lt;/li&gt;
&lt;li&gt;twemproxy更改配置文件需要重新启动，比较坑，需要修改代码使其支持动态加载。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无法动态扩容&lt;/strong&gt;，如果需要实现这个功能，要么自己写迁移脚本，手动迁移，比较繁琐，还会影响到当前服务的正常运行。或者二次开发，增加对zookeeper的依赖，将redis节点信息以及hash域相关的数据存储在zookeeper上，然后增加动态迁移数据的模块，可以在不影响现有服务运行的情况下完成增删实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;redis-cluster:51fa3b005f880190963d473247382d62&#34;&gt;Redis Cluster&lt;/h3&gt;

&lt;h4 id=&#34;数据分布-预分片:51fa3b005f880190963d473247382d62&#34;&gt;数据分布：预分片&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-redis-cluster-pre-sharding.png&#34; alt=&#34;redis-cluster-pre-sharding&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;预先分配好 16384 个slot&lt;/li&gt;
&lt;li&gt;slot 和 server 的映射关系存储每一个 server 的路由表中&lt;/li&gt;
&lt;li&gt;根据 CRC16(key) mod 16384 的值，决定将一个key放到哪一个slot中&lt;/li&gt;
&lt;li&gt;数据迁移时就是调整 slot 的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;架构-去中心化:51fa3b005f880190963d473247382d62&#34;&gt;架构：去中心化&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-redis-cluster-architecture.png&#34; alt=&#34;redis-cluster-architecture&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无中心结构，每个节点都保存数据和整个集群的状态。&lt;/li&gt;
&lt;li&gt;采用 gossip 协议传播信息以及发现新节点（最终一致性）。

&lt;ul&gt;
&lt;li&gt;每个节点都和其他所有节点连接，并保持活跃。&lt;/li&gt;
&lt;li&gt;PING/PONG：心跳，附加上自己以及一些其他节点数据，每个节点每秒随机PING几个节点。会选择那些超过cluster-node-timeout一半的时间还未PING过或未收到PONG的节点。&lt;/li&gt;
&lt;li&gt;UPDATE消息：计数戳，如果收到server的计数为3，自己的为4，则发UPDATE更新对方路由表，反之更新自己的路由表，最终集群路由状态会和计数戳最大的实例一样。&lt;/li&gt;
&lt;li&gt;如果 cluster-node-timeout 设置较小，或者节点较多，数据传输量将比较可观。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Broadcast：有状态变动时先broadcast，后PING； 发布/订阅。&lt;/li&gt;
&lt;li&gt;Redis node 不作为client请求的代理（不转发请求），client根据node返回的错误信息重定向请求?（需要 smart-client 支持），所以client连接集群中任意一个节点都可以。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;可用性-master-slave:51fa3b005f880190963d473247382d62&#34;&gt;可用性：Master-Slave&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;每个Redis Node可以有一个或者多个Slave，当Master挂掉时，选举一个Slave形成新的Master。&lt;/li&gt;
&lt;li&gt;Master Slave 之间异步复制（可能会丢数据）。&lt;/li&gt;
&lt;li&gt;采用 gossip 协议探测其他节点存活状态，超过 cluster-node-timeout，标记为 PFAIL，PING中附加此数据。当 Node A发现半数以上master将失效节点标记为PFAIL，将其标记为FAIL，broadcast FAIL。&lt;/li&gt;
&lt;li&gt;各 slave 等待一个随机时间后 发起选举，向其他 master broadcast，半数以上同意则赢得选举否则发起下一次选举&lt;/li&gt;
&lt;li&gt;当 slave 成为 master，先broadcast，后持续PING，最终集群实例都获知此消息&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;存在的问题:51fa3b005f880190963d473247382d62&#34;&gt;存在的问题&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Gossip协议通信开销&lt;/li&gt;
&lt;li&gt;严重依赖于smart-client的成熟度

&lt;ul&gt;
&lt;li&gt;如果smart-client支持缓存slot路由，需要额外占用内存空间，为了效率需要建立和所有 redis server 的长连接（每一个使用该库的程序都需要建立这么多连接）。&lt;/li&gt;
&lt;li&gt;如果不支持缓存路由信息，会先访问任意一台 redis server，之后重定向到新的节点。&lt;/li&gt;
&lt;li&gt;需要更新当前所有的client。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;官方只提供了一个ruby程序 redis-trib 完成集群的所有操作，缺乏监控管理工具，很难清楚目前集群的状态&lt;/li&gt;
&lt;li&gt;数据迁移以Key为单位，速度较慢&lt;/li&gt;
&lt;li&gt;某些操作不支持，MultiOp和Pipeline都被限定在命令中的所有Key必须都在同一Slot内&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;codis:51fa3b005f880190963d473247382d62&#34;&gt;Codis&lt;/h3&gt;

&lt;h4 id=&#34;what-is-codis:51fa3b005f880190963d473247382d62&#34;&gt;What is Codis ？&lt;/h4&gt;

&lt;p&gt;Go语言开发的分布式 Redis 解决方案，对于上层的应用来说，访问 codis 和原生的 redis server 没有明显区别（不支持发布订阅等某些命令，支持 pipeline）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7xs9f1.com1.z0.glb.clouddn.com/pic/2015/2015-09-15-redis-cluster-survey-codis-architecture.png&#34; alt=&#34;codis-architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Codis由四部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Codis Proxy (codis-proxy)&lt;/li&gt;
&lt;li&gt;Codis Dashboard (codis-config)&lt;/li&gt;
&lt;li&gt;Codis Redis (codis-server)&lt;/li&gt;
&lt;li&gt;ZooKeeper/Etcd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;codis-proxy 是客户端连接的 Redis 代理服务, codis-proxy 本身实现了 Redis 协议, 表现得和一个原生的 Redis 没什么区别 (就像 Twemproxy), 对于一个业务来说, 可以部署多个 codis-proxy, codis-proxy 本身是无状态的。&lt;/p&gt;

&lt;p&gt;codis-config 是 Codis 的管理工具, 支持包括, 添加/删除 Redis 节点, 添加/删除 Proxy 节点, 发起数据迁移等操作. codis-config 本身还自带了一个 http server, 会启动一个 dashboard, 用户可以直接在浏览器上观察 Codis 集群的运行状态。&lt;/p&gt;

&lt;p&gt;codis-server 是 Codis 项目维护的一个 Redis 分支, 基于 2.8.21 开发, 加入了 slot 的支持和原子的数据迁移指令. Codis 上层的 codis-proxy 和 codis-config 只能和这个版本的 Redis 交互才能正常运行。&lt;/p&gt;

&lt;p&gt;Codis 依赖 ZooKeeper 来存放数据路由表和 codis-proxy 节点的元信息, codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 codis-proxy。&lt;/p&gt;

&lt;p&gt;Codis 支持按照 Namespace 区分不同的产品, 拥有不同的 product name 的产品, 各项配置都不会冲突。&lt;/p&gt;

&lt;h4 id=&#34;整体设计:51fa3b005f880190963d473247382d62&#34;&gt;整体设计&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;预分片，1024 slot， key =&amp;gt; crc32(key)%1024&lt;/li&gt;
&lt;li&gt;proxy无状态，便于负载均衡，启动时在 Zookeeper 上注册一个临时节点，方便做 HA&lt;/li&gt;
&lt;li&gt;Redis 只作为存储引擎&lt;/li&gt;
&lt;li&gt;Go语言开发，可以充分利用多核，不必像 twemproxy 一样部署很多套&lt;/li&gt;
&lt;li&gt;性能损失，在不开启pipeline的情况下会损失大概40%，通过加实例线性扩展&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>
